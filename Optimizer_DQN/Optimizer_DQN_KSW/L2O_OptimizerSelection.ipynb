{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import torch.amp as amp\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ✅ GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def get_dataloader(batch_size=128):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    # MNIST 데이터셋 로드\n",
    "    mnist_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    data_loader = DataLoader(mnist_dataset, batch_size=len(mnist_dataset), shuffle=False)\n",
    "    \n",
    "    # 전체 데이터셋 가져오기\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    # 2D로 변환 (batch_size, 28*28)\n",
    "    images = images.view(images.size(0), -1).numpy()\n",
    "    \n",
    "    # Random Projection 적용 (784 -> 48)\n",
    "    rp = GaussianRandomProjection(n_components=48)\n",
    "    images_reduced = rp.fit_transform(images)\n",
    "    \n",
    "    # Tensor 변환\n",
    "    images_reduced = torch.tensor(images_reduced, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    # 새로운 DataLoader 생성\n",
    "    dataset = TensorDataset(images_reduced, labels)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim= 48, hidden_dim= 48, output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with amp.autocast(device_type=str(device)):    \n",
    "            x = self.fc1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerEnv(gym.Env):\n",
    "    def __init__(self, device):\n",
    "        super(OptimizerEnv, self).__init__()\n",
    "        self.device = device\n",
    "        self.train_loader = get_dataloader(batch_size=128)\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(5)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        # 2층 MLP 모델\n",
    "        self.model = MLP().to(self.device)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.episode_step = 0\n",
    "        self.init_loss = None  \n",
    "        self.optimizer = None  \n",
    "        self.smooth_loss = None\n",
    "        self.pca = PCA(n_components=100)\n",
    "        self.rp = GaussianRandomProjection(n_components=100)\n",
    "        self.loss_history = []\n",
    "\n",
    "        # 🔥 AMP GradScaler 추가\n",
    "        self.scaler = amp.GradScaler()\n",
    "        \n",
    "        # 🔥 데이터 로더 이터레이터 설정 (배치 단위 처리를 위해)\n",
    "        self.train_loader_iter = iter(self.train_loader)\n",
    "\n",
    "    def get_WG(self):\n",
    "        \"\"\"현재 MLP의 전체 파라미터와 Gradient를 100차원으로 축소 후 반환.\"\"\"\n",
    "        weights = []\n",
    "        gradients = []\n",
    "        \n",
    "        for param in self.model.parameters():\n",
    "            weights.append(param.data.view(-1))\n",
    "            if param.grad is not None:\n",
    "                gradients.append(param.grad.data.view(-1))\n",
    "            else:\n",
    "                gradients.append(torch.zeros_like(param.data.view(-1)))\n",
    "        \n",
    "        weights = torch.cat(weights).cpu().numpy()\n",
    "        gradients = torch.cat(gradients).cpu().numpy()\n",
    "        \n",
    "        reduced_weights = self.rp.fit_transform(weights.reshape(1, -1)) # 2842 -> 100 축소\n",
    "        reduced_gradients = self.rp.fit_transform(gradients.reshape(1, -1)) \n",
    "        \n",
    "        return reduced_weights, reduced_gradients\n",
    "\n",
    "    def step(self, action):\n",
    "        self._set_optimizer(action)\n",
    "        \"\"\"🔥 배치 단위로 옵티마이저 선택\"\"\"\n",
    "    # def step(self):     \n",
    "        # self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.RMSprop(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.Adagrad(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.SGD(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        try:\n",
    "            images, labels = next(self.train_loader_iter)  # 🔥 다음 배치 가져오기\n",
    "        except StopIteration:\n",
    "            # 🔥 배치 다 돌면 새로운 epoch 시작\n",
    "            self.train_loader_iter = iter(self.train_loader)\n",
    "            images, labels = next(self.train_loader_iter)\n",
    "\n",
    "        images, labels = images.view(images.size(0), -1).to(self.device), labels.to(self.device)\n",
    "\n",
    "        # ✅ 자동 혼합 정밀도 적용\n",
    "        with amp.autocast(device_type=str(self.device)):\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        self.loss_history.append(loss_value)\n",
    "        moving_avg_loss = np.mean(self.loss_history) if self.loss_history else 0\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # ✅ Scaler를 사용하여 역전파\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "\n",
    "        Weight, Gradient = self.get_WG()\n",
    "\n",
    "        state = np.concatenate([self.episode_step, moving_avg_loss, Weight[0], Gradient[0]])\n",
    "        self.episode_step += 1\n",
    "\n",
    "        if self.init_loss is None:\n",
    "            self.init_loss = loss_value # 🔥 초기 손실값 저장\n",
    "\n",
    "        reward = -np.log(loss_value / self.init_loss) / self.episode_step - 1 if self.episode_step >= 2 else 0 # 🔥 보상 계산\n",
    "\n",
    "        done = self.episode_step >= 10 * len(self.train_loader)  # 🔥 전체 배치 수 고려한 종료 조건\n",
    "        \n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"환경 초기화\"\"\"\n",
    "        self.episode_step = 0\n",
    "        self.model.apply(self._reset_weights)\n",
    "        self.prev_loss = None  \n",
    "        self.train_loader_iter = iter(self.train_loader)  # 🔥 새로운 epoch을 위해 데이터 로더 초기화\n",
    "        state = np.array([self.episode_step, 0.0], dtype=np.float32)\n",
    "        return state\n",
    "\n",
    "    def _set_optimizer(self, action):\n",
    "        \"\"\"매 배치마다 옵티마이저 변경\"\"\"\n",
    "        optimizers = [\n",
    "            optim.Adam(self.model.parameters(), lr=0.001),\n",
    "            optim.RMSprop(self.model.parameters(), lr=0.001),\n",
    "            optim.Adagrad(self.model.parameters(), lr=0.001),\n",
    "            optim.SGD(self.model.parameters(), lr=0.001),\n",
    "            optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        ]\n",
    "        self.optimizer = optimizers[action]\n",
    "    \n",
    "    def _reset_weights(self, m):\n",
    "        \"\"\"모델 가중치 초기화\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.reset_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, device):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = device\n",
    "\n",
    "        self.epsilon = 1.0  \n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.gamma = 0.9\n",
    "        self.lr = 0.001\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, action_size)\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.memory = []\n",
    "        self.memory_size = 5000  # 🔥 경험 저장 크기 설정\n",
    "        self.memory = deque(maxlen=self.memory_size)  # 🔥 가장 오래된 데이터를 자동 삭제\n",
    "        \n",
    "        self.scaler = amp.GradScaler()\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"ε-greedy 정책으로 행동 선택\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.model(state)).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"경험 저장\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self, batch_size=128):\n",
    "        \"\"\"DQN 학습\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "\n",
    "        with amp.autocast(device_type=str(self.device)):\n",
    "            q_values = self.model(states).gather(1, actions)\n",
    "            next_q_values = self.model(next_states).max(1, keepdim=True)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "            loss = F.mse_loss(q_values, target_q_values.detach())\n",
    "\n",
    "        # ✅ Scaler 적용\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksw00\\AppData\\Local\\Temp\\ipykernel_4332\\3017935787.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)  \u001b[38;5;66;03m# 🔥 매 batch마다 옵티마이저 선택\u001b[39;00m\n\u001b[0;32m     24\u001b[0m optimizer_count[action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# 🔥 선택된 옵티마이저 카운트 증가\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# next_state, reward, done, _ = env.step()\u001b[39;00m\n\u001b[0;32m     28\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# 🔥 reward를 episode 단위로 누적\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[46], line 84\u001b[0m, in \u001b[0;36mOptimizerEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     82\u001b[0m Weight, Gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_WG()\n\u001b[1;32m---> 84\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmoving_avg_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWeight\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGradient\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = OptimizerEnv(device)\n",
    "agent = DQNAgent(state_size=4, action_size=5, device=device)\n",
    "\n",
    "loss_history = []\n",
    "num_episodes = 1000\n",
    "\n",
    "# 🔥 옵티마이저 선택 횟수 기록\n",
    "optimizer_names = [\"Adam\", \"RMSprop\", \"Adagrad\", \"SGD\", \"Momentum\"]\n",
    "\n",
    "reward_history = []  # 🔥 각 episode의 총 reward 저장 리스트\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_losses = []\n",
    "    episode_trajectory = []\n",
    "    total_reward = 0  # 🔥 각 episode의 총 reward 초기화\n",
    "    \n",
    "    optimizer_count = {i: 0 for i in range(5)}  # 🔥 옵티마이저 선택 횟수 초기화\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)  # 🔥 매 batch마다 옵티마이저 선택\n",
    "        optimizer_count[action] += 1  # 🔥 선택된 옵티마이저 카운트 증가\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # next_state, reward, done, _ = env.step()\n",
    "\n",
    "        total_reward += reward  # 🔥 reward를 episode 단위로 누적\n",
    "        episode_losses.append(state['moving_avg_loss'])  \n",
    "        episode_trajectory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    for transition in episode_trajectory:\n",
    "        agent.remember(*transition)\n",
    "\n",
    "    avg_loss = sum(episode_losses) / len(episode_losses)\n",
    "    loss_history.append(avg_loss)\n",
    "    reward_history.append(total_reward)  # 🔥 episode 종료 후 총 reward 저장\n",
    "\n",
    "    agent.train()\n",
    "\n",
    "    print(f\"Episode {episode+1}, Avg Loss: {avg_loss}, Total Reward: {total_reward}\")\n",
    "    print(f\"Optimizer Counts: {', '.join([f'{optimizer_names[i]}: {optimizer_count[i]}' for i in range(5)])}\")\n",
    "\n",
    "# plt.plot(loss_history, label=\"L2O Optimizer Loss\")\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Loss Reduction Over Training Episodes\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward 수정\n",
    "- moving average 방식\n",
    "- 배치단위 리워드 -> 에폭 단위 리워드로 변경?\n",
    "state 수정\n",
    "- 파라미터 값 추가\n",
    "loss 설정의 문제?\n",
    "\n",
    "학습이 잘 되고있음을 확인할 수 있는 Loss, Reward 지표 수정\n",
    "\n",
    "1개 선택 -- 각각 episode 1개 10번씩 정확도 평균\n",
    "- Adam : 95.717%\n",
    "- RMSprop : 93.448%\n",
    "- Adagrad : 95.785%\n",
    "- SGD : 87.355%\n",
    "- Momentum : 87.296%\n",
    "\n",
    "5개 선택\n",
    "- 1:40, 92.82%, 900, batch size 64\n",
    "- 3.14 1000episode, momentum 우세, 테스트 정확도 88.01%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.01%\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "model = env.model\n",
    "# 모델 평가 함수\n",
    "def evaluate_model(model, testloader):\n",
    "    model.eval()  # 평가 모드 활성화\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.view(images.size(0), -1).to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # 가장 높은 확률의 클래스 선택\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# 모델 정확성 평가\n",
    "evaluate_model(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/L20_Learned_MLP.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 학습된 DQN 에이전트 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# 🔥 학습이 끝난 후 모델과 옵티마이저 저장\n",
    "torch.save({\n",
    "    'model_state_dict': agent.model.state_dict(),\n",
    "    'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "    'epsilon': agent.epsilon  # Epsilon 값도 저장 (탐색 비율 유지)\n",
    "}, \"dqn_agent.pth\")\n",
    "\n",
    "print(\"✅ 학습된 DQN 에이전트 저장 완료!\")\n",
    "\n",
    "def load_agent(agent, checkpoint_path=\"dqn_agent.pth\"):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=agent.device)\n",
    "    agent.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    agent.epsilon = checkpoint['epsilon']  # 저장된 탐색 비율 로드\n",
    "    print(\"✅ 저장된 DQN 에이전트 로드 완료!\")\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장된 DQN 에이전트 로드 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksw00\\AppData\\Local\\Temp\\ipykernel_22292\\3414198289.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=agent.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 평가 완료! 총 보상(Total Reward): -17.18892593183535\n"
     ]
    }
   ],
   "source": [
    "# 🔥 새로운 환경 생성\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = OptimizerEnv(device)\n",
    "\n",
    "# 🔥 새로운 DQNAgent 생성 (같은 구조로 초기화)\n",
    "agent = DQNAgent(state_size=2, action_size=5, device=device)\n",
    "\n",
    "# 🔥 저장된 모델 불러오기\n",
    "agent = load_agent(agent)\n",
    "\n",
    "# 🔥 모델 평가 (탐색 없이 greedy 행동)\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = torch.argmax(agent.model(torch.FloatTensor(state).unsqueeze(0).to(agent.device))).item()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "print(f\"🎯 평가 완료! 총 보상(Total Reward): {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
