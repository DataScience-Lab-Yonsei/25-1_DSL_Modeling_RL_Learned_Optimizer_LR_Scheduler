{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import torch.amp as amp\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# âœ… GPU ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def get_dataloader(batch_size=128):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    # MNIST ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "    mnist_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    data_loader = DataLoader(mnist_dataset, batch_size=len(mnist_dataset), shuffle=False)\n",
    "    \n",
    "    # ì „ì²´ ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    # 2Dë¡œ ë³€í™˜ (batch_size, 28*28)\n",
    "    images = images.view(images.size(0), -1).numpy()\n",
    "    \n",
    "    # Random Projection ì ìš© (784 -> 48)\n",
    "    rp = GaussianRandomProjection(n_components=48)\n",
    "    images_reduced = rp.fit_transform(images)\n",
    "    \n",
    "    # Tensor ë³€í™˜\n",
    "    images_reduced = torch.tensor(images_reduced, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    # ìƒˆë¡œìš´ DataLoader ìƒì„±\n",
    "    dataset = TensorDataset(images_reduced, labels)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim= 48, hidden_dim= 48, output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with amp.autocast(device_type=str(device)):    \n",
    "            x = self.fc1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerEnv(gym.Env):\n",
    "    def __init__(self, device):\n",
    "        super(OptimizerEnv, self).__init__()\n",
    "        self.device = device\n",
    "        self.train_loader = get_dataloader(batch_size=128)\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(5)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        # 2ì¸µ MLP ëª¨ë¸\n",
    "        self.model = MLP().to(self.device)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.episode_step = 0\n",
    "        self.init_loss = None  \n",
    "        self.optimizer = None  \n",
    "        self.smooth_loss = None\n",
    "        self.pca = PCA(n_components=100)\n",
    "        self.rp = GaussianRandomProjection(n_components=100)\n",
    "        self.loss_history = []\n",
    "\n",
    "        # ğŸ”¥ AMP GradScaler ì¶”ê°€\n",
    "        self.scaler = amp.GradScaler()\n",
    "        \n",
    "        # ğŸ”¥ ë°ì´í„° ë¡œë” ì´í„°ë ˆì´í„° ì„¤ì • (ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ë¥¼ ìœ„í•´)\n",
    "        self.train_loader_iter = iter(self.train_loader)\n",
    "\n",
    "    def get_WG(self):\n",
    "        \"\"\"í˜„ì¬ MLPì˜ ì „ì²´ íŒŒë¼ë¯¸í„°ì™€ Gradientë¥¼ 100ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ í›„ ë°˜í™˜.\"\"\"\n",
    "        weights = []\n",
    "        gradients = []\n",
    "        \n",
    "        for param in self.model.parameters():\n",
    "            weights.append(param.data.view(-1))\n",
    "            if param.grad is not None:\n",
    "                gradients.append(param.grad.data.view(-1))\n",
    "            else:\n",
    "                gradients.append(torch.zeros_like(param.data.view(-1)))\n",
    "        \n",
    "        weights = torch.cat(weights).cpu().numpy()\n",
    "        gradients = torch.cat(gradients).cpu().numpy()\n",
    "        \n",
    "        reduced_weights = self.rp.fit_transform(weights.reshape(1, -1)) # 2842 -> 100 ì¶•ì†Œ\n",
    "        reduced_gradients = self.rp.fit_transform(gradients.reshape(1, -1)) \n",
    "        \n",
    "        return reduced_weights, reduced_gradients\n",
    "\n",
    "    def step(self, action):\n",
    "        self._set_optimizer(action)\n",
    "        \"\"\"ğŸ”¥ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì˜µí‹°ë§ˆì´ì € ì„ íƒ\"\"\"\n",
    "    # def step(self):     \n",
    "        # self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.RMSprop(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.Adagrad(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.SGD(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        try:\n",
    "            images, labels = next(self.train_loader_iter)  # ğŸ”¥ ë‹¤ìŒ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°\n",
    "        except StopIteration:\n",
    "            # ğŸ”¥ ë°°ì¹˜ ë‹¤ ëŒë©´ ìƒˆë¡œìš´ epoch ì‹œì‘\n",
    "            self.train_loader_iter = iter(self.train_loader)\n",
    "            images, labels = next(self.train_loader_iter)\n",
    "\n",
    "        images, labels = images.view(images.size(0), -1).to(self.device), labels.to(self.device)\n",
    "\n",
    "        # âœ… ìë™ í˜¼í•© ì •ë°€ë„ ì ìš©\n",
    "        with amp.autocast(device_type=str(self.device)):\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        self.loss_history.append(loss_value)\n",
    "        moving_avg_loss = np.mean(self.loss_history) if self.loss_history else 0\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # âœ… Scalerë¥¼ ì‚¬ìš©í•˜ì—¬ ì—­ì „íŒŒ\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "\n",
    "        Weight, Gradient = self.get_WG()\n",
    "\n",
    "        state = np.concatenate([self.episode_step, moving_avg_loss, Weight[0], Gradient[0]])\n",
    "        self.episode_step += 1\n",
    "\n",
    "        if self.init_loss is None:\n",
    "            self.init_loss = loss_value # ğŸ”¥ ì´ˆê¸° ì†ì‹¤ê°’ ì €ì¥\n",
    "\n",
    "        reward = -np.log(loss_value / self.init_loss) / self.episode_step - 1 if self.episode_step >= 2 else 0 # ğŸ”¥ ë³´ìƒ ê³„ì‚°\n",
    "\n",
    "        done = self.episode_step >= 10 * len(self.train_loader)  # ğŸ”¥ ì „ì²´ ë°°ì¹˜ ìˆ˜ ê³ ë ¤í•œ ì¢…ë£Œ ì¡°ê±´\n",
    "        \n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"í™˜ê²½ ì´ˆê¸°í™”\"\"\"\n",
    "        self.episode_step = 0\n",
    "        self.model.apply(self._reset_weights)\n",
    "        self.prev_loss = None  \n",
    "        self.train_loader_iter = iter(self.train_loader)  # ğŸ”¥ ìƒˆë¡œìš´ epochì„ ìœ„í•´ ë°ì´í„° ë¡œë” ì´ˆê¸°í™”\n",
    "        state = np.array([self.episode_step, 0.0], dtype=np.float32)\n",
    "        return state\n",
    "\n",
    "    def _set_optimizer(self, action):\n",
    "        \"\"\"ë§¤ ë°°ì¹˜ë§ˆë‹¤ ì˜µí‹°ë§ˆì´ì € ë³€ê²½\"\"\"\n",
    "        optimizers = [\n",
    "            optim.Adam(self.model.parameters(), lr=0.001),\n",
    "            optim.RMSprop(self.model.parameters(), lr=0.001),\n",
    "            optim.Adagrad(self.model.parameters(), lr=0.001),\n",
    "            optim.SGD(self.model.parameters(), lr=0.001),\n",
    "            optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        ]\n",
    "        self.optimizer = optimizers[action]\n",
    "    \n",
    "    def _reset_weights(self, m):\n",
    "        \"\"\"ëª¨ë¸ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.reset_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, device):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = device\n",
    "\n",
    "        self.epsilon = 1.0  \n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.gamma = 0.9\n",
    "        self.lr = 0.001\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, action_size)\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.memory = []\n",
    "        self.memory_size = 5000  # ğŸ”¥ ê²½í—˜ ì €ì¥ í¬ê¸° ì„¤ì •\n",
    "        self.memory = deque(maxlen=self.memory_size)  # ğŸ”¥ ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°ë¥¼ ìë™ ì‚­ì œ\n",
    "        \n",
    "        self.scaler = amp.GradScaler()\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Îµ-greedy ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.model(state)).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"ê²½í—˜ ì €ì¥\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self, batch_size=128):\n",
    "        \"\"\"DQN í•™ìŠµ\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "\n",
    "        with amp.autocast(device_type=str(self.device)):\n",
    "            q_values = self.model(states).gather(1, actions)\n",
    "            next_q_values = self.model(next_states).max(1, keepdim=True)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "            loss = F.mse_loss(q_values, target_q_values.detach())\n",
    "\n",
    "        # âœ… Scaler ì ìš©\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksw00\\AppData\\Local\\Temp\\ipykernel_4332\\3017935787.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)  \u001b[38;5;66;03m# ğŸ”¥ ë§¤ batchë§ˆë‹¤ ì˜µí‹°ë§ˆì´ì € ì„ íƒ\u001b[39;00m\n\u001b[0;32m     24\u001b[0m optimizer_count[action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# ğŸ”¥ ì„ íƒëœ ì˜µí‹°ë§ˆì´ì € ì¹´ìš´íŠ¸ ì¦ê°€\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# next_state, reward, done, _ = env.step()\u001b[39;00m\n\u001b[0;32m     28\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# ğŸ”¥ rewardë¥¼ episode ë‹¨ìœ„ë¡œ ëˆ„ì \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[46], line 84\u001b[0m, in \u001b[0;36mOptimizerEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     82\u001b[0m Weight, Gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_WG()\n\u001b[1;32m---> 84\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmoving_avg_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWeight\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGradient\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = OptimizerEnv(device)\n",
    "agent = DQNAgent(state_size=4, action_size=5, device=device)\n",
    "\n",
    "loss_history = []\n",
    "num_episodes = 1000\n",
    "\n",
    "# ğŸ”¥ ì˜µí‹°ë§ˆì´ì € ì„ íƒ íšŸìˆ˜ ê¸°ë¡\n",
    "optimizer_names = [\"Adam\", \"RMSprop\", \"Adagrad\", \"SGD\", \"Momentum\"]\n",
    "\n",
    "reward_history = []  # ğŸ”¥ ê° episodeì˜ ì´ reward ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_losses = []\n",
    "    episode_trajectory = []\n",
    "    total_reward = 0  # ğŸ”¥ ê° episodeì˜ ì´ reward ì´ˆê¸°í™”\n",
    "    \n",
    "    optimizer_count = {i: 0 for i in range(5)}  # ğŸ”¥ ì˜µí‹°ë§ˆì´ì € ì„ íƒ íšŸìˆ˜ ì´ˆê¸°í™”\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)  # ğŸ”¥ ë§¤ batchë§ˆë‹¤ ì˜µí‹°ë§ˆì´ì € ì„ íƒ\n",
    "        optimizer_count[action] += 1  # ğŸ”¥ ì„ íƒëœ ì˜µí‹°ë§ˆì´ì € ì¹´ìš´íŠ¸ ì¦ê°€\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # next_state, reward, done, _ = env.step()\n",
    "\n",
    "        total_reward += reward  # ğŸ”¥ rewardë¥¼ episode ë‹¨ìœ„ë¡œ ëˆ„ì \n",
    "        episode_losses.append(state['moving_avg_loss'])  \n",
    "        episode_trajectory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    for transition in episode_trajectory:\n",
    "        agent.remember(*transition)\n",
    "\n",
    "    avg_loss = sum(episode_losses) / len(episode_losses)\n",
    "    loss_history.append(avg_loss)\n",
    "    reward_history.append(total_reward)  # ğŸ”¥ episode ì¢…ë£Œ í›„ ì´ reward ì €ì¥\n",
    "\n",
    "    agent.train()\n",
    "\n",
    "    print(f\"Episode {episode+1}, Avg Loss: {avg_loss}, Total Reward: {total_reward}\")\n",
    "    print(f\"Optimizer Counts: {', '.join([f'{optimizer_names[i]}: {optimizer_count[i]}' for i in range(5)])}\")\n",
    "\n",
    "# plt.plot(loss_history, label=\"L2O Optimizer Loss\")\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Loss Reduction Over Training Episodes\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward ìˆ˜ì •\n",
    "- moving average ë°©ì‹\n",
    "- ë°°ì¹˜ë‹¨ìœ„ ë¦¬ì›Œë“œ -> ì—í­ ë‹¨ìœ„ ë¦¬ì›Œë“œë¡œ ë³€ê²½?\n",
    "state ìˆ˜ì •\n",
    "- íŒŒë¼ë¯¸í„° ê°’ ì¶”ê°€\n",
    "loss ì„¤ì •ì˜ ë¬¸ì œ?\n",
    "\n",
    "í•™ìŠµì´ ì˜ ë˜ê³ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆëŠ” Loss, Reward ì§€í‘œ ìˆ˜ì •\n",
    "\n",
    "1ê°œ ì„ íƒ -- ê°ê° episode 1ê°œ 10ë²ˆì”© ì •í™•ë„ í‰ê· \n",
    "- Adam : 95.717%\n",
    "- RMSprop : 93.448%\n",
    "- Adagrad : 95.785%\n",
    "- SGD : 87.355%\n",
    "- Momentum : 87.296%\n",
    "\n",
    "5ê°œ ì„ íƒ\n",
    "- 1:40, 92.82%, 900, batch size 64\n",
    "- 3.14 1000episode, momentum ìš°ì„¸, í…ŒìŠ¤íŠ¸ ì •í™•ë„ 88.01%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.01%\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "model = env.model\n",
    "# ëª¨ë¸ í‰ê°€ í•¨ìˆ˜\n",
    "def evaluate_model(model, testloader):\n",
    "    model.eval()  # í‰ê°€ ëª¨ë“œ í™œì„±í™”\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.view(images.size(0), -1).to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# ëª¨ë¸ ì •í™•ì„± í‰ê°€\n",
    "evaluate_model(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/L20_Learned_MLP.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í•™ìŠµëœ DQN ì—ì´ì „íŠ¸ ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¥ í•™ìŠµì´ ëë‚œ í›„ ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì € ì €ì¥\n",
    "torch.save({\n",
    "    'model_state_dict': agent.model.state_dict(),\n",
    "    'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "    'epsilon': agent.epsilon  # Epsilon ê°’ë„ ì €ì¥ (íƒìƒ‰ ë¹„ìœ¨ ìœ ì§€)\n",
    "}, \"dqn_agent.pth\")\n",
    "\n",
    "print(\"âœ… í•™ìŠµëœ DQN ì—ì´ì „íŠ¸ ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "def load_agent(agent, checkpoint_path=\"dqn_agent.pth\"):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=agent.device)\n",
    "    agent.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    agent.epsilon = checkpoint['epsilon']  # ì €ì¥ëœ íƒìƒ‰ ë¹„ìœ¨ ë¡œë“œ\n",
    "    print(\"âœ… ì €ì¥ëœ DQN ì—ì´ì „íŠ¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ëœ DQN ì—ì´ì „íŠ¸ ë¡œë“œ ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksw00\\AppData\\Local\\Temp\\ipykernel_22292\\3414198289.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=agent.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ í‰ê°€ ì™„ë£Œ! ì´ ë³´ìƒ(Total Reward): -17.18892593183535\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¥ ìƒˆë¡œìš´ í™˜ê²½ ìƒì„±\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = OptimizerEnv(device)\n",
    "\n",
    "# ğŸ”¥ ìƒˆë¡œìš´ DQNAgent ìƒì„± (ê°™ì€ êµ¬ì¡°ë¡œ ì´ˆê¸°í™”)\n",
    "agent = DQNAgent(state_size=2, action_size=5, device=device)\n",
    "\n",
    "# ğŸ”¥ ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "agent = load_agent(agent)\n",
    "\n",
    "# ğŸ”¥ ëª¨ë¸ í‰ê°€ (íƒìƒ‰ ì—†ì´ greedy í–‰ë™)\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = torch.argmax(agent.model(torch.FloatTensor(state).unsqueeze(0).to(agent.device))).item()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "print(f\"ğŸ¯ í‰ê°€ ì™„ë£Œ! ì´ ë³´ìƒ(Total Reward): {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
