{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import torch.amp as amp\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ✅ GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_dataloader(batch_size=32, train = True):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    if train:\n",
    "        # MNIST 데이터셋 로드\n",
    "        mnist_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "        data_loader = DataLoader(mnist_dataset, batch_size=len(mnist_dataset), shuffle=False)\n",
    "    else:\n",
    "        # MNIST 데이터셋 로드\n",
    "        test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "        data_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    \n",
    "    # 전체 데이터셋 가져오기\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    # 2D로 변환 (batch_size, 28*28)\n",
    "    images = images.view(images.size(0), -1).numpy()\n",
    "    \n",
    "    # Random Projection 적용 (784 -> 48)\n",
    "    rp = GaussianRandomProjection(n_components=48)\n",
    "    images_reduced = rp.fit_transform(images)\n",
    "    \n",
    "    # Tensor 변환\n",
    "    images_reduced = torch.tensor(images_reduced, dtype=torch.float32)\n",
    "    labels = labels.clone().detach().long()\n",
    "    \n",
    "    # 새로운 DataLoader 생성\n",
    "    dataset = TensorDataset(images_reduced, labels)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return data_loader\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim= 48, hidden_dim= 48, output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with amp.autocast(device_type=str(device)):    \n",
    "            x = self.fc1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "testloader = get_dataloader(batch_size=128, train=False)\n",
    "\n",
    "# 모델 평가 함수\n",
    "def evaluate_model(model, testloader):\n",
    "    model.eval()  # 평가 모드 활성화\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.view(images.size(0), -1).to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # 가장 높은 확률의 클래스 선택\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerEnv(gym.Env):\n",
    "    def __init__(self, device):\n",
    "        super(OptimizerEnv, self).__init__()\n",
    "        self.device = device\n",
    "        self.train_loader = get_dataloader(batch_size=128, train=True)\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(5)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(202,), dtype=np.float32)\n",
    "\n",
    "        # 2층 MLP 모델\n",
    "        self.model = MLP().to(self.device)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.episode_step = 0\n",
    "        self.init_loss = None   \n",
    "        self.optimizer = None  \n",
    "        self.smooth_loss = None\n",
    "        self.loss_history = []\n",
    "        self.rp_w = GaussianRandomProjection(n_components=100)\n",
    "        self.rp_g = GaussianRandomProjection(n_components=100)\n",
    "        # 🔥 AMP GradScaler 추가\n",
    "        self.scaler = amp.GradScaler()\n",
    "        # 🔥 데이터 로더 이터레이터 설정 (배치 단위 처리를 위해)\n",
    "        self.train_loader_iter = iter(self.train_loader)\n",
    "    \n",
    "    # def step(self, action):\n",
    "        # self._set_optimizer(action)\n",
    "        \"\"\"🔥 배치 단위로 옵티마이저 선택\"\"\"\n",
    "    def step(self):     \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.RMSprop(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.Adagrad(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.SGD(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        try:\n",
    "            images, labels = next(self.train_loader_iter)  # 🔥 다음 배치 가져오기\n",
    "        except StopIteration:\n",
    "            # 🔥 배치 다 돌면 새로운 epoch 시작\n",
    "            self.train_loader_iter = iter(self.train_loader)\n",
    "            images, labels = next(self.train_loader_iter)\n",
    "\n",
    "        images, labels = images.view(images.size(0), -1).to(self.device), labels.to(self.device)\n",
    "\n",
    "        # ✅ 자동 혼합 정밀도 적용\n",
    "        with amp.autocast(device_type=str(self.device)):\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        self.loss_history.append(loss_value)\n",
    "        moving_avg_loss = np.mean(self.loss_history) if self.loss_history else 0\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # ✅ Scaler를 사용하여 역전파\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        \n",
    "        # 🔥 MLP 모델의 가중치를 벡터로 변환\n",
    "        model_params = torch.cat([p.flatten() for p in self.model.parameters()]).detach().cpu().numpy()\n",
    "\n",
    "        # 🔥 MLP 모델의 그라디언트를 벡터로 변환 (없으면 0으로 채움)\n",
    "        model_grads = []\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                model_grads.append(p.grad.flatten())\n",
    "            else:\n",
    "                model_grads.append(torch.zeros_like(p.flatten()))  # 🔥 None이면 0으로 채움\n",
    "        model_grads = torch.cat(model_grads).detach().cpu().numpy()\n",
    "        model_grads = np.nan_to_num(model_grads, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        # 🔥 첫 step에서 Random Projection 학습\n",
    "        if self.episode_step == 0:\n",
    "            self.rp_w.fit(model_params.reshape(1, -1))\n",
    "            self.rp_g.fit(model_grads.reshape(1, -1))\n",
    "\n",
    "        # 🔥 가중치 및 그라디언트를 Random Projection을 통해 100차원으로 축소\n",
    "        reduced_params = self.rp_w.transform(model_params.reshape(1, -1)).flatten()\n",
    "        reduced_grads = self.rp_g.transform(model_grads.reshape(1, -1)).flatten()\n",
    "\n",
    "        \n",
    "\n",
    "        # 🔥 state에 축소된 모델 가중치 및 그라디언트 추가\n",
    "        state = np.concatenate(([self.episode_step, moving_avg_loss], reduced_params, reduced_grads)).astype(np.float32)\n",
    "        self.episode_step += 1\n",
    "\n",
    "        if self.init_loss is None:\n",
    "            self.init_loss = loss_value # 🔥 초기 손실값 저장\n",
    "\n",
    "        reward = -np.log(loss_value / self.init_loss) / self.episode_step - 1 if self.episode_step >= 2 else 0 # 🔥 보상 계산\n",
    "  \n",
    "        done = self.episode_step >= 10 * len(self.train_loader)  # 🔥 전체 배치 수 고려한 종료 조건\n",
    "        \n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"환경 초기화\"\"\"\n",
    "        self.episode_step = 0\n",
    "        self.model.apply(self._reset_weights)\n",
    "        self.train_loader_iter = iter(self.train_loader)\n",
    "\n",
    "        # 🔥 초기 가중치 및 그라디언트를 0으로 설정 (완전 초기화)\n",
    "        reduced_params = np.zeros(100, dtype=np.float32)  # 가중치 초기값을 0으로 설정\n",
    "        reduced_grads = np.zeros(100, dtype=np.float32)  # 초기에는 그라디언트 없음\n",
    "\n",
    "        # 🔥 state에 축소된 가중치와 초기 그라디언트 포함\n",
    "        state = np.concatenate(([self.episode_step, 0.0], reduced_params, reduced_grads)).astype(np.float32)\n",
    "        return state\n",
    "\n",
    "\n",
    "    def _set_optimizer(self, action):\n",
    "        \"\"\"매 배치마다 옵티마이저 변경\"\"\"\n",
    "        optimizers = [\n",
    "            optim.Adam(self.model.parameters(), lr=0.001),\n",
    "            optim.RMSprop(self.model.parameters(), lr=0.001),\n",
    "            optim.Adagrad(self.model.parameters(), lr=0.001),\n",
    "            optim.SGD(self.model.parameters(), lr=0.001),\n",
    "            optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        ]\n",
    "        self.optimizer = optimizers[action]\n",
    "    \n",
    "    def _reset_weights(self, m):\n",
    "        \"\"\"모델 가중치 초기화\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, device):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = device\n",
    "\n",
    "        self.epsilon = 1.0  \n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.gamma = 0.9\n",
    "        self.lr = 0.001\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, action_size)\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.memory = []\n",
    "        self.memory_size = 5000  # 🔥 경험 저장 크기 설정\n",
    "        self.memory = deque(maxlen=self.memory_size)  # 🔥 가장 오래된 데이터를 자동 삭제\n",
    "\n",
    "        self.scaler = amp.GradScaler()\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"ε-greedy 정책으로 행동 선택\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.model(state)).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"경험 저장\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self, batch_size=128):\n",
    "        \"\"\"DQN 학습\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "\n",
    "        with amp.autocast(device_type=str(self.device)):\n",
    "            q_values = self.model(states).gather(1, actions)\n",
    "            next_q_values = self.model(next_states).max(1, keepdim=True)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "            loss = F.mse_loss(q_values, target_q_values.detach())\n",
    "\n",
    "        # ✅ Scaler 적용\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Avg Loss: 0.5890555665182915, Total Reward: -4681.4114604612305\n",
      "Optimizer Counts: Adam: 890, RMSprop: 994, Adagrad: 896, SGD: 962, Momentum: 948\n",
      "Test Accuracy: 10.66%\n",
      "Episode 2, Avg Loss: 0.4316786947344412, Total Reward: -4681.622147729182\n",
      "Optimizer Counts: Adam: 969, RMSprop: 947, Adagrad: 926, SGD: 935, Momentum: 913\n",
      "Test Accuracy: 12.14%\n",
      "Episode 3, Avg Loss: 0.4194138993141748, Total Reward: -4681.3503441829835\n",
      "Optimizer Counts: Adam: 921, RMSprop: 904, Adagrad: 965, SGD: 948, Momentum: 952\n",
      "Test Accuracy: 10.40%\n",
      "Episode 4, Avg Loss: 0.4157713455559094, Total Reward: -4681.542719725404\n",
      "Optimizer Counts: Adam: 910, RMSprop: 947, Adagrad: 944, SGD: 949, Momentum: 940\n",
      "Test Accuracy: 10.33%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m optimizer_count[action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# 🔥 선택된 옵티마이저 카운트 증가\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# next_state, reward, done, _ = env.step(action)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# 🔥 reward를 episode 단위로 누적\u001b[39;00m\n\u001b[0;32m     29\u001b[0m episode_losses\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;241m1\u001b[39m])  \n",
      "Cell \u001b[1;32mIn[70], line 47\u001b[0m, in \u001b[0;36mOptimizerEnv.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)):\n\u001b[0;32m     46\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(images)\n\u001b[1;32m---> 47\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_history\u001b[38;5;241m.\u001b[39mappend(loss_value)\n",
      "File \u001b[1;32mc:\\Users\\ksw00\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksw00\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ksw00\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksw00\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = OptimizerEnv(device)\n",
    "agent = DQNAgent(state_size=202, action_size=5, device=device)\n",
    "\n",
    "loss_history = []\n",
    "num_episodes = 10\n",
    "\n",
    "# 🔥 옵티마이저 선택 횟수 기록\n",
    "optimizer_names = [\"Adam\", \"RMSprop\", \"Adagrad\", \"SGD\", \"Momentum\"]\n",
    "\n",
    "reward_history = []  # 🔥 각 episode의 총 reward 저장 리스트\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_losses = []\n",
    "    episode_trajectory = []\n",
    "    total_reward = 0  # 🔥 각 episode의 총 reward 초기화\n",
    "    \n",
    "    optimizer_count = {i: 0 for i in range(5)}  # 🔥 옵티마이저 선택 횟수 초기화\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)  # 🔥 매 batch마다 옵티마이저 선택\n",
    "        optimizer_count[action] += 1  # 🔥 선택된 옵티마이저 카운트 증가\n",
    "        # next_state, reward, done, _ = env.step(action)\n",
    "        next_state, reward, done, _ = env.step()\n",
    "\n",
    "        total_reward += reward  # 🔥 reward를 episode 단위로 누적\n",
    "        episode_losses.append(state[1])  \n",
    "        episode_trajectory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    for transition in episode_trajectory:\n",
    "        agent.remember(*transition)\n",
    "\n",
    "    avg_loss = sum(episode_losses) / len(episode_losses)\n",
    "    loss_history.append(avg_loss)\n",
    "    reward_history.append(total_reward)  # 🔥 episode 종료 후 총 reward 저장\n",
    "\n",
    "    print(f\"Episode {episode+1}, Avg Loss: {avg_loss}, Total Reward: {total_reward}\")\n",
    "    print(f\"Optimizer Counts: {', '.join([f'{optimizer_names[i]}: {optimizer_count[i]}' for i in range(5)])}\")\n",
    "\n",
    "    model = env.model\n",
    "    # 모델 정확성 평가\n",
    "    evaluate_model(model, testloader)\n",
    "\n",
    "# plt.plot(loss_history, label=\"L2O Optimizer Loss\")\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Loss Reduction Over Training Episodes\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward 수정\n",
    "- moving average 방식\n",
    "- 배치단위 리워드 -> 에폭 단위 리워드로 변경?\n",
    "state 수정\n",
    "- 파라미터 값 추가\n",
    "loss 설정의 문제?\n",
    "\n",
    "학습이 잘 되고있음을 확인할 수 있는 Loss, Reward 지표 수정\n",
    "\n",
    "1개 선택 -- 각각 episode 1개 10번씩 정확도 확인\n",
    "- adam,     94.66%, 95.39%, 96.23%, 95.77%, 96.36%, 95.60%, 96.50%, 95.00%, 96.09%, 95.57%  >>  95.717%\n",
    "- RMSprop,  93.47%, 92.14%, 95.06%, 92.66%, 95.20%, 94.15%, 94.18%, 91.92%, 91.90%, 93.80%  >>  93.448%\n",
    "- Adagrad,  95.51%, 96.14%, 95.67%, 95.64%, 95.97%, 96.06%, 96.28%, 96.00%, 95.54%, 95.04%  >>  95.785%\n",
    "- SGD,      87.30%, 87.15%, 87.48%, 87.30%, 87.41%, 87.51%, 87.64%, 87.37%, 87.35%, 87.04%  >>  87.355%\n",
    "- Momentum, 87.47%, 87.37%, 87.40%, 87.15%, 87.13%, 87.16%, 87.50%, 87.23%, 87.29%, 87.26%  >>  87.296%\n",
    "\n",
    "5개 선택\n",
    "- 1:40, 92.82%\n",
    "- Episode 1, Avg Loss: 1.3594644563784921, Total Reward: -3.892620003062391\n",
    "- Optimizer Counts: Adam: 928, RMSprop: 908, Adagrad: 929, SGD: 989, Momentum: 936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/L20_Learned_MLP.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksw00\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    850\u001b[0m         _save(\n\u001b[0;32m    851\u001b[0m             obj,\n\u001b[0;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    856\u001b[0m         )\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ksw00\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksw00\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory models does not exist."
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"models/L20_Learned_MLP.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 학습된 DQN 에이전트 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# 🔥 학습이 끝난 후 모델과 옵티마이저 저장\n",
    "torch.save({\n",
    "    'model_state_dict': agent.model.state_dict(),\n",
    "    'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "    'epsilon': agent.epsilon  # Epsilon 값도 저장 (탐색 비율 유지)\n",
    "}, \"dqn_agent.pth\")\n",
    "\n",
    "print(\"✅ 학습된 DQN 에이전트 저장 완료!\")\n",
    "\n",
    "def load_agent(agent, checkpoint_path=\"dqn_agent.pth\"):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=agent.device)\n",
    "    agent.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    agent.epsilon = checkpoint['epsilon']  # 저장된 탐색 비율 로드\n",
    "    print(\"✅ 저장된 DQN 에이전트 로드 완료!\")\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장된 DQN 에이전트 로드 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksw00\\AppData\\Local\\Temp\\ipykernel_22292\\3414198289.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=agent.device)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "OptimizerEnv.step() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     17\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(agent\u001b[38;5;241m.\u001b[39mmodel(torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(agent\u001b[38;5;241m.\u001b[39mdevice)))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 18\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     20\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "\u001b[1;31mTypeError\u001b[0m: OptimizerEnv.step() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# 🔥 새로운 환경 생성\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = OptimizerEnv(device)\n",
    "\n",
    "# 🔥 새로운 DQNAgent 생성 (같은 구조로 초기화)\n",
    "agent = DQNAgent(state_size=2, action_size=5, device=device)\n",
    "\n",
    "# 🔥 저장된 모델 불러오기\n",
    "agent = load_agent(agent)\n",
    "\n",
    "# 🔥 모델 평가 (탐색 없이 greedy 행동)\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = torch.argmax(agent.model(torch.FloatTensor(state).unsqueeze(0).to(agent.device))).item()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "print(f\"🎯 평가 완료! 총 보상(Total Reward): {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
