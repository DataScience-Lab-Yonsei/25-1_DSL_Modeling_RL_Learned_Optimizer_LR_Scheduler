{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import torch.amp as amp\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# âœ… GPU ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_dataloader(batch_size=32, train = True):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    if train:\n",
    "        # MNIST ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "        mnist_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "        data_loader = DataLoader(mnist_dataset, batch_size=len(mnist_dataset), shuffle=False)\n",
    "    else:\n",
    "        # MNIST ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "        test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "        data_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    \n",
    "    # ì „ì²´ ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    # 2Dë¡œ ë³€í™˜ (batch_size, 28*28)\n",
    "    images = images.view(images.size(0), -1).numpy()\n",
    "    \n",
    "    # Random Projection ì ìš© (784 -> 48)\n",
    "    rp = GaussianRandomProjection(n_components=48)\n",
    "    images_reduced = rp.fit_transform(images)\n",
    "    \n",
    "    # Tensor ë³€í™˜\n",
    "    images_reduced = torch.tensor(images_reduced, dtype=torch.float32)\n",
    "    labels = labels.clone().detach().long()\n",
    "    \n",
    "    # ìƒˆë¡œìš´ DataLoader ìƒì„±\n",
    "    dataset = TensorDataset(images_reduced, labels)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return data_loader\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim= 48, hidden_dim= 48, output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with amp.autocast(device_type=str(device)):    \n",
    "            x = self.fc1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "testloader = get_dataloader(batch_size=128, train=False)\n",
    "\n",
    "# ëª¨ë¸ í‰ê°€ í•¨ìˆ˜\n",
    "def evaluate_model(model, testloader):\n",
    "    model.eval()  # í‰ê°€ ëª¨ë“œ í™œì„±í™”\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.view(images.size(0), -1).to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerEnv(gym.Env):\n",
    "    def __init__(self, device):\n",
    "        super(OptimizerEnv, self).__init__()\n",
    "        self.device = device\n",
    "        self.train_loader = get_dataloader(batch_size=128, train=True)\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(5)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(202,), dtype=np.float32)\n",
    "\n",
    "        # 2ì¸µ MLP ëª¨ë¸\n",
    "        self.model = MLP().to(self.device)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.episode_step = 0\n",
    "        self.init_loss = None   \n",
    "        self.optimizer = None  \n",
    "        self.smooth_loss = None\n",
    "        self.loss_history = []\n",
    "        self.rp_w = GaussianRandomProjection(n_components=100)\n",
    "        self.rp_g = GaussianRandomProjection(n_components=100)\n",
    "        # ğŸ”¥ AMP GradScaler ì¶”ê°€\n",
    "        self.scaler = amp.GradScaler()\n",
    "        # ğŸ”¥ ë°ì´í„° ë¡œë” ì´í„°ë ˆì´í„° ì„¤ì • (ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ë¥¼ ìœ„í•´)\n",
    "        self.train_loader_iter = iter(self.train_loader)\n",
    "    \n",
    "    # def step(self, action):\n",
    "        # self._set_optimizer(action)\n",
    "        \"\"\"ğŸ”¥ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì˜µí‹°ë§ˆì´ì € ì„ íƒ\"\"\"\n",
    "    def step(self):     \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.RMSprop(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.Adagrad(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.SGD(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        try:\n",
    "            images, labels = next(self.train_loader_iter)  # ğŸ”¥ ë‹¤ìŒ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°\n",
    "        except StopIteration:\n",
    "            # ğŸ”¥ ë°°ì¹˜ ë‹¤ ëŒë©´ ìƒˆë¡œìš´ epoch ì‹œì‘\n",
    "            self.train_loader_iter = iter(self.train_loader)\n",
    "            images, labels = next(self.train_loader_iter)\n",
    "\n",
    "        images, labels = images.view(images.size(0), -1).to(self.device), labels.to(self.device)\n",
    "\n",
    "        # âœ… ìë™ í˜¼í•© ì •ë°€ë„ ì ìš©\n",
    "        with amp.autocast(device_type=str(self.device)):\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        self.loss_history.append(loss_value)\n",
    "        moving_avg_loss = np.mean(self.loss_history) if self.loss_history else 0\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # âœ… Scalerë¥¼ ì‚¬ìš©í•˜ì—¬ ì—­ì „íŒŒ\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        \n",
    "        # ğŸ”¥ MLP ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
    "        model_params = torch.cat([p.flatten() for p in self.model.parameters()]).detach().cpu().numpy()\n",
    "\n",
    "        # ğŸ”¥ MLP ëª¨ë¸ì˜ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€)\n",
    "        model_grads = []\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                model_grads.append(p.grad.flatten())\n",
    "            else:\n",
    "                model_grads.append(torch.zeros_like(p.flatten()))  # ğŸ”¥ Noneì´ë©´ 0ìœ¼ë¡œ ì±„ì›€\n",
    "        model_grads = torch.cat(model_grads).detach().cpu().numpy()\n",
    "        model_grads = np.nan_to_num(model_grads, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        # ğŸ”¥ ì²« stepì—ì„œ Random Projection í•™ìŠµ\n",
    "        if self.episode_step == 0:\n",
    "            self.rp_w.fit(model_params.reshape(1, -1))\n",
    "            self.rp_g.fit(model_grads.reshape(1, -1))\n",
    "\n",
    "        # ğŸ”¥ ê°€ì¤‘ì¹˜ ë° ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ Random Projectionì„ í†µí•´ 100ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ\n",
    "        reduced_params = self.rp_w.transform(model_params.reshape(1, -1)).flatten()\n",
    "        reduced_grads = self.rp_g.transform(model_grads.reshape(1, -1)).flatten()\n",
    "\n",
    "        \n",
    "\n",
    "        # ğŸ”¥ stateì— ì¶•ì†Œëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë° ê·¸ë¼ë””ì–¸íŠ¸ ì¶”ê°€\n",
    "        state = np.concatenate(([self.episode_step, moving_avg_loss], reduced_params, reduced_grads)).astype(np.float32)\n",
    "        self.episode_step += 1\n",
    "\n",
    "        if self.init_loss is None:\n",
    "            self.init_loss = loss_value # ğŸ”¥ ì´ˆê¸° ì†ì‹¤ê°’ ì €ì¥\n",
    "\n",
    "        reward = -np.log(loss_value / self.init_loss) / self.episode_step - 1 if self.episode_step >= 2 else 0 # ğŸ”¥ ë³´ìƒ ê³„ì‚°\n",
    "  \n",
    "        done = self.episode_step >= 10 * len(self.train_loader)  # ğŸ”¥ ì „ì²´ ë°°ì¹˜ ìˆ˜ ê³ ë ¤í•œ ì¢…ë£Œ ì¡°ê±´\n",
    "        \n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"í™˜ê²½ ì´ˆê¸°í™”\"\"\"\n",
    "        self.episode_step = 0\n",
    "        self.model.apply(self._reset_weights)\n",
    "        self.train_loader_iter = iter(self.train_loader)\n",
    "\n",
    "        # ğŸ”¥ ì´ˆê¸° ê°€ì¤‘ì¹˜ ë° ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ 0ìœ¼ë¡œ ì„¤ì • (ì™„ì „ ì´ˆê¸°í™”)\n",
    "        reduced_params = np.zeros(100, dtype=np.float32)  # ê°€ì¤‘ì¹˜ ì´ˆê¸°ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •\n",
    "        reduced_grads = np.zeros(100, dtype=np.float32)  # ì´ˆê¸°ì—ëŠ” ê·¸ë¼ë””ì–¸íŠ¸ ì—†ìŒ\n",
    "\n",
    "        # ğŸ”¥ stateì— ì¶•ì†Œëœ ê°€ì¤‘ì¹˜ì™€ ì´ˆê¸° ê·¸ë¼ë””ì–¸íŠ¸ í¬í•¨\n",
    "        state = np.concatenate(([self.episode_step, 0.0], reduced_params, reduced_grads)).astype(np.float32)\n",
    "        return state\n",
    "\n",
    "\n",
    "    def _set_optimizer(self, action):\n",
    "        \"\"\"ë§¤ ë°°ì¹˜ë§ˆë‹¤ ì˜µí‹°ë§ˆì´ì € ë³€ê²½\"\"\"\n",
    "        optimizers = [\n",
    "            optim.Adam(self.model.parameters(), lr=0.001),\n",
    "            optim.RMSprop(self.model.parameters(), lr=0.001),\n",
    "            optim.Adagrad(self.model.parameters(), lr=0.001),\n",
    "            optim.SGD(self.model.parameters(), lr=0.001),\n",
    "            optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        ]\n",
    "        self.optimizer = optimizers[action]\n",
    "    \n",
    "    def _reset_weights(self, m):\n",
    "        \"\"\"ëª¨ë¸ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, device):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = device\n",
    "\n",
    "        self.epsilon = 1.0  \n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.gamma = 0.9\n",
    "        self.lr = 0.001\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, action_size)\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.memory = []\n",
    "        self.memory_size = 5000  # ğŸ”¥ ê²½í—˜ ì €ì¥ í¬ê¸° ì„¤ì •\n",
    "        self.memory = deque(maxlen=self.memory_size)  # ğŸ”¥ ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°ë¥¼ ìë™ ì‚­ì œ\n",
    "\n",
    "        self.scaler = amp.GradScaler()\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Îµ-greedy ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.model(state)).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"ê²½í—˜ ì €ì¥\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self, batch_size=128):\n",
    "        \"\"\"DQN í•™ìŠµ\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "\n",
    "        with amp.autocast(device_type=str(self.device)):\n",
    "            q_values = self.model(states).gather(1, actions)\n",
    "            next_q_values = self.model(next_states).max(1, keepdim=True)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "            loss = F.mse_loss(q_values, target_q_values.detach())\n",
    "\n",
    "        # âœ… Scaler ì ìš©\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Avg Loss: 0.5890555665182915, Total Reward: -4681.4114604612305\n",
      "Optimizer Counts: Adam: 890, RMSprop: 994, Adagrad: 896, SGD: 962, Momentum: 948\n",
      "Test Accuracy: 10.66%\n",
      "Episode 2, Avg Loss: 0.4316786947344412, Total Reward: -4681.622147729182\n",
      "Optimizer Counts: Adam: 969, RMSprop: 947, Adagrad: 926, SGD: 935, Momentum: 913\n",
      "Test Accuracy: 12.14%\n",
      "Episode 3, Avg Loss: 0.4194138993141748, Total Reward: -4681.3503441829835\n",
      "Optimizer Counts: Adam: 921, RMSprop: 904, Adagrad: 965, SGD: 948, Momentum: 952\n",
      "Test Accuracy: 10.40%\n",
      "Episode 4, Avg Loss: 0.4157713455559094, Total Reward: -4681.542719725404\n",
      "Optimizer Counts: Adam: 910, RMSprop: 947, Adagrad: 944, SGD: 949, Momentum: 940\n",
      "Test Accuracy: 10.33%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m optimizer_count[action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# ğŸ”¥ ì„ íƒëœ ì˜µí‹°ë§ˆì´ì € ì¹´ìš´íŠ¸ ì¦ê°€\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# next_state, reward, done, _ = env.step(action)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# ğŸ”¥ rewardë¥¼ episode ë‹¨ìœ„ë¡œ ëˆ„ì \u001b[39;00m\n\u001b[0;32m     29\u001b[0m episode_losses\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;241m1\u001b[39m])  \n",
      "Cell \u001b[1;32mIn[70], line 47\u001b[0m, in \u001b[0;36mOptimizerEnv.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)):\n\u001b[0;32m     46\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(images)\n\u001b[1;32m---> 47\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_history\u001b[38;5;241m.\u001b[39mappend(loss_value)\n",
      "File \u001b[1;32mc:\\Users\\ksw00\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksw00\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ksw00\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksw00\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = OptimizerEnv(device)\n",
    "agent = DQNAgent(state_size=202, action_size=5, device=device)\n",
    "\n",
    "loss_history = []\n",
    "num_episodes = 10\n",
    "\n",
    "# ğŸ”¥ ì˜µí‹°ë§ˆì´ì € ì„ íƒ íšŸìˆ˜ ê¸°ë¡\n",
    "optimizer_names = [\"Adam\", \"RMSprop\", \"Adagrad\", \"SGD\", \"Momentum\"]\n",
    "\n",
    "reward_history = []  # ğŸ”¥ ê° episodeì˜ ì´ reward ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_losses = []\n",
    "    episode_trajectory = []\n",
    "    total_reward = 0  # ğŸ”¥ ê° episodeì˜ ì´ reward ì´ˆê¸°í™”\n",
    "    \n",
    "    optimizer_count = {i: 0 for i in range(5)}  # ğŸ”¥ ì˜µí‹°ë§ˆì´ì € ì„ íƒ íšŸìˆ˜ ì´ˆê¸°í™”\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)  # ğŸ”¥ ë§¤ batchë§ˆë‹¤ ì˜µí‹°ë§ˆì´ì € ì„ íƒ\n",
    "        optimizer_count[action] += 1  # ğŸ”¥ ì„ íƒëœ ì˜µí‹°ë§ˆì´ì € ì¹´ìš´íŠ¸ ì¦ê°€\n",
    "        # next_state, reward, done, _ = env.step(action)\n",
    "        next_state, reward, done, _ = env.step()\n",
    "\n",
    "        total_reward += reward  # ğŸ”¥ rewardë¥¼ episode ë‹¨ìœ„ë¡œ ëˆ„ì \n",
    "        episode_losses.append(state[1])  \n",
    "        episode_trajectory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    for transition in episode_trajectory:\n",
    "        agent.remember(*transition)\n",
    "\n",
    "    avg_loss = sum(episode_losses) / len(episode_losses)\n",
    "    loss_history.append(avg_loss)\n",
    "    reward_history.append(total_reward)  # ğŸ”¥ episode ì¢…ë£Œ í›„ ì´ reward ì €ì¥\n",
    "\n",
    "    print(f\"Episode {episode+1}, Avg Loss: {avg_loss}, Total Reward: {total_reward}\")\n",
    "    print(f\"Optimizer Counts: {', '.join([f'{optimizer_names[i]}: {optimizer_count[i]}' for i in range(5)])}\")\n",
    "\n",
    "    model = env.model\n",
    "    # ëª¨ë¸ ì •í™•ì„± í‰ê°€\n",
    "    evaluate_model(model, testloader)\n",
    "\n",
    "# plt.plot(loss_history, label=\"L2O Optimizer Loss\")\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Loss Reduction Over Training Episodes\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward ìˆ˜ì •\n",
    "- moving average ë°©ì‹\n",
    "- ë°°ì¹˜ë‹¨ìœ„ ë¦¬ì›Œë“œ -> ì—í­ ë‹¨ìœ„ ë¦¬ì›Œë“œë¡œ ë³€ê²½?\n",
    "state ìˆ˜ì •\n",
    "- íŒŒë¼ë¯¸í„° ê°’ ì¶”ê°€\n",
    "loss ì„¤ì •ì˜ ë¬¸ì œ?\n",
    "\n",
    "í•™ìŠµì´ ì˜ ë˜ê³ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆëŠ” Loss, Reward ì§€í‘œ ìˆ˜ì •\n",
    "\n",
    "1ê°œ ì„ íƒ -- ê°ê° episode 1ê°œ 10ë²ˆì”© ì •í™•ë„ í™•ì¸\n",
    "- adam,     94.66%, 95.39%, 96.23%, 95.77%, 96.36%, 95.60%, 96.50%, 95.00%, 96.09%, 95.57%  >>  95.717%\n",
    "- RMSprop,  93.47%, 92.14%, 95.06%, 92.66%, 95.20%, 94.15%, 94.18%, 91.92%, 91.90%, 93.80%  >>  93.448%\n",
    "- Adagrad,  95.51%, 96.14%, 95.67%, 95.64%, 95.97%, 96.06%, 96.28%, 96.00%, 95.54%, 95.04%  >>  95.785%\n",
    "- SGD,      87.30%, 87.15%, 87.48%, 87.30%, 87.41%, 87.51%, 87.64%, 87.37%, 87.35%, 87.04%  >>  87.355%\n",
    "- Momentum, 87.47%, 87.37%, 87.40%, 87.15%, 87.13%, 87.16%, 87.50%, 87.23%, 87.29%, 87.26%  >>  87.296%\n",
    "\n",
    "5ê°œ ì„ íƒ\n",
    "- 1:40, 92.82%\n",
    "- Episode 1, Avg Loss: 1.3594644563784921, Total Reward: -3.892620003062391\n",
    "- Optimizer Counts: Adam: 928, RMSprop: 908, Adagrad: 929, SGD: 989, Momentum: 936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/L20_Learned_MLP.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksw00\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    850\u001b[0m         _save(\n\u001b[0;32m    851\u001b[0m             obj,\n\u001b[0;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    856\u001b[0m         )\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ksw00\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksw00\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory models does not exist."
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"models/L20_Learned_MLP.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í•™ìŠµëœ DQN ì—ì´ì „íŠ¸ ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¥ í•™ìŠµì´ ëë‚œ í›„ ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì € ì €ì¥\n",
    "torch.save({\n",
    "    'model_state_dict': agent.model.state_dict(),\n",
    "    'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "    'epsilon': agent.epsilon  # Epsilon ê°’ë„ ì €ì¥ (íƒìƒ‰ ë¹„ìœ¨ ìœ ì§€)\n",
    "}, \"dqn_agent.pth\")\n",
    "\n",
    "print(\"âœ… í•™ìŠµëœ DQN ì—ì´ì „íŠ¸ ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "def load_agent(agent, checkpoint_path=\"dqn_agent.pth\"):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=agent.device)\n",
    "    agent.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    agent.epsilon = checkpoint['epsilon']  # ì €ì¥ëœ íƒìƒ‰ ë¹„ìœ¨ ë¡œë“œ\n",
    "    print(\"âœ… ì €ì¥ëœ DQN ì—ì´ì „íŠ¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ëœ DQN ì—ì´ì „íŠ¸ ë¡œë“œ ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksw00\\AppData\\Local\\Temp\\ipykernel_22292\\3414198289.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=agent.device)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "OptimizerEnv.step() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     17\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(agent\u001b[38;5;241m.\u001b[39mmodel(torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(agent\u001b[38;5;241m.\u001b[39mdevice)))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 18\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     20\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "\u001b[1;31mTypeError\u001b[0m: OptimizerEnv.step() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# ğŸ”¥ ìƒˆë¡œìš´ í™˜ê²½ ìƒì„±\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = OptimizerEnv(device)\n",
    "\n",
    "# ğŸ”¥ ìƒˆë¡œìš´ DQNAgent ìƒì„± (ê°™ì€ êµ¬ì¡°ë¡œ ì´ˆê¸°í™”)\n",
    "agent = DQNAgent(state_size=2, action_size=5, device=device)\n",
    "\n",
    "# ğŸ”¥ ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "agent = load_agent(agent)\n",
    "\n",
    "# ğŸ”¥ ëª¨ë¸ í‰ê°€ (íƒìƒ‰ ì—†ì´ greedy í–‰ë™)\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = torch.argmax(agent.model(torch.FloatTensor(state).unsqueeze(0).to(agent.device))).item()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "print(f\"ğŸ¯ í‰ê°€ ì™„ë£Œ! ì´ ë³´ìƒ(Total Reward): {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
