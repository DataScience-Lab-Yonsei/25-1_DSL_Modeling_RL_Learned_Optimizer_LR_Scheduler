{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "import torch.amp as amp\n",
    "\n",
    "data_path = \"C:/Users/ksw00/mycode/data\"\n",
    "# ✅ GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "\n",
    "def get_dataloader(batch_size, train):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    if train:\n",
    "        # MNIST 데이터셋 로드\n",
    "        mnist_dataset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n",
    "        data_loader = DataLoader(mnist_dataset, batch_size=len(mnist_dataset), shuffle=False)\n",
    "    else:\n",
    "        # MNIST 데이터셋 로드\n",
    "        test_dataset = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)\n",
    "        data_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    \n",
    "    # # RP\n",
    "    # data_iter = iter(data_loader)\n",
    "    # images, labels = next(data_iter)\n",
    "    # images = images.view(images.size(0), -1).numpy()\n",
    "    \n",
    "    # rp = GaussianRandomProjection(n_components=784)\n",
    "    # images_reduced = rp.fit_transform(images)\n",
    "\n",
    "    # eps = 1e-8\n",
    "    # images_reduced = (images_reduced - np.mean(images_reduced, axis=0)) / (np.std(images_reduced, axis=0) + eps)\n",
    "\n",
    "    # images_reduced = torch.tensor(images_reduced, dtype=torch.float32)\n",
    "    # labels = labels.clone().detach().long()\n",
    "\n",
    "    # dataset = TensorDataset(images_reduced, labels)\n",
    "    # data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=train)\n",
    "\n",
    "\n",
    "    # No RP\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    dataset = TensorDataset(images, labels)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle = train)\n",
    "    \n",
    "    return data_loader\n",
    "\n",
    "testloader = get_dataloader(batch_size, train=False)\n",
    "\n",
    "# 모델 평가 함수\n",
    "def evaluate_model(model, testloader):\n",
    "    model.eval()  # 평가 모드 활성화\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.view(images.size(0), -1).to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # 가장 높은 확률의 클래스 선택\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim= 48, hidden_dim= 48, output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        with amp.autocast(device_type=str(device)):   \n",
    "            x = self.fc1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class MLP2(nn.Module):\n",
    "    def __init__(self, input_dim= 28*28, hidden_dim= 128, output_dim=10):\n",
    "        super(MLP2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):    \n",
    "        with amp.autocast(device_type=str(device)): \n",
    "            x = self.fc1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerEnv(gym.Env):\n",
    "    def __init__(self, device, batch_size):\n",
    "        super(OptimizerEnv, self).__init__()\n",
    "        self.device = device\n",
    "        self.train_loader = get_dataloader(batch_size, train=True)\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(5)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "        # 2층 MLP 모델\n",
    "        # self.model = MLP().to(self.device)\n",
    "        self.model = MLP2().to(self.device)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.episode_step = 0\n",
    "        self.init_loss = None  \n",
    "        self.optimizer = None  \n",
    "        self.loss_history = []\n",
    "        # 🔥 AMP GradScaler 추가\n",
    "        self.scaler = amp.GradScaler()\n",
    "        # 🔥 데이터 로더 이터레이터 설정 (배치 단위 처리를 위해)\n",
    "        self.train_loader_iter = iter(self.train_loader)\n",
    "\n",
    "    def step(self, action):\n",
    "        self._set_optimizer(action)\n",
    "        \"\"\"🔥 배치 단위로 옵티마이저 선택\"\"\"\n",
    "    # def step(self):     \n",
    "    #     self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.RMSprop(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.Adagrad(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.SGD(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        \n",
    "        try:\n",
    "            images, labels = next(self.train_loader_iter)  # 🔥 다음 배치 가져오기\n",
    "        except StopIteration:\n",
    "            # 🔥 배치 다 돌면 새로운 epoch 시작\n",
    "            self.train_loader_iter = iter(self.train_loader)\n",
    "            images, labels = next(self.train_loader_iter)\n",
    "\n",
    "        images, labels = images.view(images.size(0), -1).to(self.device), labels.to(self.device)\n",
    "\n",
    "        # ✅ 자동 혼합 정밀도 적용\n",
    "        with amp.autocast(device_type=str(self.device)):\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        self.loss_history.append(loss_value)\n",
    "        moving_avg_loss = np.mean(self.loss_history[-10:]) if self.loss_history else 0\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # ✅ Scaler를 사용하여 역전파\n",
    "        self.scaler.scale(loss).backward()\n",
    "\n",
    "        # 🔥 MLP 모델의 가중치를 벡터로 변환\n",
    "        model_params = torch.cat([p.flatten() for p in self.model.parameters()]).detach().cpu().numpy()\n",
    "\n",
    "        # 🔥 MLP 모델의 그라디언트를 벡터로 변환 (없으면 0으로 채움)\n",
    "        model_grads = []\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                model_grads.append(p.grad.flatten())\n",
    "            else:\n",
    "                model_grads.append(torch.zeros_like(p.flatten()))  # 🔥 None이면 0으로 채움\n",
    "        model_grads = torch.cat(model_grads).detach().cpu().numpy()\n",
    "        model_grads = np.nan_to_num(model_grads, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        \n",
    "        # 🔥 state에 축소된 모델 가중치 및 그라디언트 추가\n",
    "        state = np.concatenate(([self.episode_step, moving_avg_loss], model_params, model_grads)).astype(np.float32)\n",
    "        self.episode_step += 1\n",
    "\n",
    "        if self.init_loss is None:\n",
    "            self.init_loss = loss_value # 🔥 초기 손실값 저장\n",
    "\n",
    "        reward = -np.log(loss_value / self.init_loss) / (self.episode_step - 1) if self.episode_step > 1 else 0\n",
    "            \n",
    "        done = self.episode_step >= 10 * len(self.train_loader)  # 🔥 전체 배치 수 고려한 종료 조건\n",
    "        \n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"환경 초기화\"\"\"\n",
    "        self.episode_step = 0\n",
    "        self.init_loss = None\n",
    "        self.loss_history = []\n",
    "        self.model.apply(self._reset_weights)\n",
    "        self.train_loader_iter = iter(self.train_loader)\n",
    "\n",
    "        # 🔥 초기 가중치 및 그라디언트를 0으로 설정 (완전 초기화)\n",
    "        # model_params = np.zeros(2842, dtype=np.float32)  # 가중치 초기값을 0으로 설정\n",
    "        # model_grads = np.zeros(2842, dtype=np.float32)  # 초기에는 그라디언트 없음\n",
    "\n",
    "        model_params = np.zeros(101770, dtype=np.float32)  # 가중치 초기값을 0으로 설정\n",
    "        model_grads = np.zeros(101770, dtype=np.float32)  # 초기에는 그라디언트 없음\n",
    "\n",
    "        # 🔥 state에 축소된 가중치와 초기 그라디언트 포함\n",
    "        state = np.concatenate(([self.episode_step, 0.0], model_params, model_grads)).astype(np.float32)\n",
    "        return state\n",
    "\n",
    "    def _set_optimizer(self, action):\n",
    "        \"\"\"매 배치마다 옵티마이저 변경\"\"\"\n",
    "        optimizers = [\n",
    "            optim.Adam(self.model.parameters(), lr=0.001),\n",
    "            optim.RMSprop(self.model.parameters(), lr=0.001),\n",
    "            optim.Adagrad(self.model.parameters(), lr=0.001),\n",
    "            optim.SGD(self.model.parameters(), lr=0.001),\n",
    "            optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        ]\n",
    "        self.optimizer = optimizers[action]\n",
    "    \n",
    "    def _reset_weights(self, m):\n",
    "        \"\"\"모델 가중치 초기화\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, device):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = device\n",
    "\n",
    "        self.epsilon = 1.0  \n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99  # 🔥 탐색 비율 감소 조정\n",
    "        self.gamma = 0.9  # 🔥 단기적 보상 고려 강화\n",
    "        self.lr = 0.001\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_size)\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.memory = []\n",
    "        self.memory_size = 5000  # 🔥 경험 저장 크기 설정\n",
    "        self.memory = deque(maxlen=self.memory_size)  # 🔥 가장 오래된 데이터를 자동 삭제\n",
    "\n",
    "        self.scaler = amp.GradScaler()\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"ε-greedy 정책으로 행동 선택\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.model(state)).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"경험 저장\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self, batch_size=128):\n",
    "        \"\"\"DQN 학습\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "\n",
    "        with amp.autocast(device_type=str(self.device)):\n",
    "            q_values = self.model(states).gather(1, actions)\n",
    "            next_q_values = self.model(next_states).max(1, keepdim=True)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "            loss = F.mse_loss(q_values, target_q_values.detach())\n",
    "\n",
    "        # ✅ Scaler 적용\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Avg Loss: 1.231890397234512, Episode mean Reward: 0.0006057805069635702\n",
      "Optimizer Counts: Adam: 980, RMSprop: 928, Adagrad: 967, SGD: 936, Momentum: 879\n",
      "Test Accuracy: 92.22%\n",
      "Episode 2, Avg Loss: 1.2956901462665245, Episode mean Reward: 0.0004323564197591837\n",
      "Optimizer Counts: Adam: 906, RMSprop: 983, Adagrad: 952, SGD: 921, Momentum: 928\n",
      "Test Accuracy: 94.09%\n",
      "Episode 3, Avg Loss: 1.1941080491680072, Episode mean Reward: 0.0005806761317231226\n",
      "Optimizer Counts: Adam: 915, RMSprop: 987, Adagrad: 907, SGD: 960, Momentum: 921\n",
      "Test Accuracy: 92.13%\n",
      "Episode 4, Avg Loss: 1.3232380460002529, Episode mean Reward: 0.0004644261116003281\n",
      "Optimizer Counts: Adam: 887, RMSprop: 1028, Adagrad: 911, SGD: 935, Momentum: 929\n",
      "Test Accuracy: 93.68%\n",
      "Episode 5, Avg Loss: 1.2564257536425012, Episode mean Reward: 0.000668385652896342\n",
      "Optimizer Counts: Adam: 908, RMSprop: 1056, Adagrad: 879, SGD: 911, Momentum: 936\n",
      "Test Accuracy: 93.21%\n",
      "Episode 6, Avg Loss: 1.1656318811401885, Episode mean Reward: 0.0004243769963253888\n",
      "Optimizer Counts: Adam: 870, RMSprop: 1039, Adagrad: 886, SGD: 898, Momentum: 997\n",
      "Test Accuracy: 92.52%\n",
      "Episode 7, Avg Loss: 1.2246570794948384, Episode mean Reward: 0.0004860107286225843\n",
      "Optimizer Counts: Adam: 892, RMSprop: 1054, Adagrad: 947, SGD: 879, Momentum: 918\n",
      "Test Accuracy: 87.07%\n",
      "Episode 8, Avg Loss: 1.3649045997464073, Episode mean Reward: 0.00040825406120742177\n",
      "Optimizer Counts: Adam: 865, RMSprop: 1071, Adagrad: 955, SGD: 857, Momentum: 942\n",
      "Test Accuracy: 88.94%\n",
      "Episode 9, Avg Loss: 1.3063602090390252, Episode mean Reward: 0.000521181281444042\n",
      "Optimizer Counts: Adam: 843, RMSprop: 1148, Adagrad: 917, SGD: 844, Momentum: 938\n",
      "Test Accuracy: 93.53%\n",
      "Episode 10, Avg Loss: 1.2577476267875638, Episode mean Reward: 0.00038048592754753624\n",
      "Optimizer Counts: Adam: 902, RMSprop: 1135, Adagrad: 907, SGD: 845, Momentum: 901\n",
      "Test Accuracy: 93.29%\n",
      "Episode 11, Avg Loss: 1.348777549099058, Episode mean Reward: 0.00040874451087057463\n",
      "Optimizer Counts: Adam: 834, RMSprop: 1087, Adagrad: 929, SGD: 938, Momentum: 902\n",
      "Test Accuracy: 92.09%\n",
      "Episode 12, Avg Loss: 1.2937167333323818, Episode mean Reward: 0.0006444224874379892\n",
      "Optimizer Counts: Adam: 857, RMSprop: 1162, Adagrad: 862, SGD: 871, Momentum: 938\n",
      "Test Accuracy: 90.69%\n",
      "Episode 13, Avg Loss: 1.380621165151535, Episode mean Reward: 0.00039720514449793524\n",
      "Optimizer Counts: Adam: 814, RMSprop: 1193, Adagrad: 890, SGD: 876, Momentum: 917\n",
      "Test Accuracy: 93.13%\n",
      "Episode 14, Avg Loss: 1.3459083367385336, Episode mean Reward: 0.00031183032864524305\n",
      "Optimizer Counts: Adam: 814, RMSprop: 1232, Adagrad: 843, SGD: 876, Momentum: 925\n",
      "Test Accuracy: 92.22%\n",
      "Episode 15, Avg Loss: 1.3226846107668968, Episode mean Reward: 0.00046142022395140997\n",
      "Optimizer Counts: Adam: 783, RMSprop: 1238, Adagrad: 871, SGD: 876, Momentum: 922\n",
      "Test Accuracy: 93.29%\n",
      "Episode 16, Avg Loss: 1.3708469572860296, Episode mean Reward: 0.00026449063805236104\n",
      "Optimizer Counts: Adam: 813, RMSprop: 1305, Adagrad: 852, SGD: 832, Momentum: 888\n",
      "Test Accuracy: 91.97%\n",
      "Episode 17, Avg Loss: 1.337719859637177, Episode mean Reward: 0.0005647927159366634\n",
      "Optimizer Counts: Adam: 808, RMSprop: 1215, Adagrad: 838, SGD: 885, Momentum: 944\n",
      "Test Accuracy: 93.63%\n",
      "Episode 18, Avg Loss: 1.2982057254832944, Episode mean Reward: 0.0005015485262273918\n",
      "Optimizer Counts: Adam: 762, RMSprop: 1364, Adagrad: 851, SGD: 777, Momentum: 936\n",
      "Test Accuracy: 93.05%\n",
      "Episode 19, Avg Loss: 1.3736161130831948, Episode mean Reward: 0.00037023132638008946\n",
      "Optimizer Counts: Adam: 775, RMSprop: 1380, Adagrad: 803, SGD: 844, Momentum: 888\n",
      "Test Accuracy: 93.30%\n",
      "Episode 20, Avg Loss: 1.4920249864744988, Episode mean Reward: 0.0004056132046139953\n",
      "Optimizer Counts: Adam: 796, RMSprop: 1390, Adagrad: 832, SGD: 806, Momentum: 866\n",
      "Test Accuracy: 90.96%\n",
      "Episode 21, Avg Loss: 1.3874303504792866, Episode mean Reward: 0.0004086178466729054\n",
      "Optimizer Counts: Adam: 742, RMSprop: 1391, Adagrad: 857, SGD: 771, Momentum: 929\n",
      "Test Accuracy: 92.97%\n",
      "Episode 22, Avg Loss: 1.261071369236212, Episode mean Reward: 0.0002983058192363945\n",
      "Optimizer Counts: Adam: 731, RMSprop: 1334, Adagrad: 852, SGD: 804, Momentum: 969\n",
      "Test Accuracy: 90.77%\n",
      "Episode 23, Avg Loss: 1.325928490680418, Episode mean Reward: 0.00033115035092869523\n",
      "Optimizer Counts: Adam: 795, RMSprop: 1417, Adagrad: 785, SGD: 808, Momentum: 885\n",
      "Test Accuracy: 91.04%\n",
      "Episode 24, Avg Loss: 1.4472479721249294, Episode mean Reward: 0.00036853520108935596\n",
      "Optimizer Counts: Adam: 741, RMSprop: 1399, Adagrad: 856, SGD: 830, Momentum: 864\n",
      "Test Accuracy: 93.58%\n",
      "Episode 25, Avg Loss: 1.4086529437539927, Episode mean Reward: 0.00020915181439587203\n",
      "Optimizer Counts: Adam: 752, RMSprop: 1418, Adagrad: 761, SGD: 785, Momentum: 974\n",
      "Test Accuracy: 94.01%\n",
      "Episode 26, Avg Loss: 1.4422199755843514, Episode mean Reward: 8.522912097472804e-05\n",
      "Optimizer Counts: Adam: 740, RMSprop: 1515, Adagrad: 770, SGD: 810, Momentum: 855\n",
      "Test Accuracy: 92.61%\n",
      "Episode 27, Avg Loss: 1.3711939559562374, Episode mean Reward: 0.0002587134201291819\n",
      "Optimizer Counts: Adam: 785, RMSprop: 1453, Adagrad: 767, SGD: 753, Momentum: 932\n",
      "Test Accuracy: 93.01%\n",
      "Episode 28, Avg Loss: 1.4051825591114793, Episode mean Reward: -1.4754974573465353e-05\n",
      "Optimizer Counts: Adam: 713, RMSprop: 1492, Adagrad: 806, SGD: 751, Momentum: 928\n",
      "Test Accuracy: 92.23%\n",
      "Episode 29, Avg Loss: 1.4530678713753789, Episode mean Reward: 0.0004632701919314188\n",
      "Optimizer Counts: Adam: 730, RMSprop: 1563, Adagrad: 755, SGD: 715, Momentum: 927\n",
      "Test Accuracy: 93.23%\n",
      "Episode 30, Avg Loss: 1.3419481252683505, Episode mean Reward: 0.00031959572680985903\n",
      "Optimizer Counts: Adam: 747, RMSprop: 1476, Adagrad: 792, SGD: 751, Momentum: 924\n",
      "Test Accuracy: 91.21%\n",
      "Episode 31, Avg Loss: 1.4002132511342258, Episode mean Reward: 0.00045607057072782827\n",
      "Optimizer Counts: Adam: 715, RMSprop: 1510, Adagrad: 735, SGD: 730, Momentum: 1000\n",
      "Test Accuracy: 89.69%\n",
      "Episode 32, Avg Loss: 1.3856278396999913, Episode mean Reward: 0.00023171092501867598\n",
      "Optimizer Counts: Adam: 688, RMSprop: 1520, Adagrad: 748, SGD: 817, Momentum: 917\n",
      "Test Accuracy: 92.41%\n",
      "Episode 33, Avg Loss: 1.4294709054010508, Episode mean Reward: 0.0003201267470967555\n",
      "Optimizer Counts: Adam: 648, RMSprop: 1626, Adagrad: 764, SGD: 744, Momentum: 908\n",
      "Test Accuracy: 94.42%\n",
      "Episode 34, Avg Loss: 1.4145870197302244, Episode mean Reward: 0.0003000192577539492\n",
      "Optimizer Counts: Adam: 679, RMSprop: 1645, Adagrad: 756, SGD: 738, Momentum: 872\n",
      "Test Accuracy: 91.28%\n",
      "Episode 35, Avg Loss: 1.4033390127138288, Episode mean Reward: 0.00035978667934205176\n",
      "Optimizer Counts: Adam: 689, RMSprop: 1623, Adagrad: 774, SGD: 720, Momentum: 884\n",
      "Test Accuracy: 92.18%\n",
      "Episode 36, Avg Loss: 1.4344588117241097, Episode mean Reward: 0.0007214664043529631\n",
      "Optimizer Counts: Adam: 608, RMSprop: 1626, Adagrad: 765, SGD: 729, Momentum: 962\n",
      "Test Accuracy: 92.48%\n",
      "Episode 37, Avg Loss: 1.3487939170428684, Episode mean Reward: 0.0003751898447766742\n",
      "Optimizer Counts: Adam: 663, RMSprop: 1632, Adagrad: 798, SGD: 665, Momentum: 932\n",
      "Test Accuracy: 88.00%\n",
      "Episode 38, Avg Loss: 1.399988896112198, Episode mean Reward: 0.00047234542257482034\n",
      "Optimizer Counts: Adam: 610, RMSprop: 1639, Adagrad: 715, SGD: 716, Momentum: 1010\n",
      "Test Accuracy: 93.40%\n",
      "Episode 39, Avg Loss: 1.3982080402913124, Episode mean Reward: 0.0002794028443346348\n",
      "Optimizer Counts: Adam: 643, RMSprop: 1636, Adagrad: 806, SGD: 720, Momentum: 885\n",
      "Test Accuracy: 90.24%\n",
      "Episode 40, Avg Loss: 1.3791176057954841, Episode mean Reward: 0.00012493456864151324\n",
      "Optimizer Counts: Adam: 580, RMSprop: 1696, Adagrad: 803, SGD: 710, Momentum: 901\n",
      "Test Accuracy: 92.75%\n",
      "Episode 41, Avg Loss: 1.472670358254203, Episode mean Reward: 0.00019106057663052957\n",
      "Optimizer Counts: Adam: 683, RMSprop: 1726, Adagrad: 746, SGD: 685, Momentum: 850\n",
      "Test Accuracy: 93.30%\n",
      "Episode 42, Avg Loss: 1.4439716819252795, Episode mean Reward: 0.0004531268112083105\n",
      "Optimizer Counts: Adam: 587, RMSprop: 1852, Adagrad: 677, SGD: 744, Momentum: 830\n",
      "Test Accuracy: 94.00%\n",
      "Episode 43, Avg Loss: 1.4611313701311408, Episode mean Reward: 0.00042258208199074906\n",
      "Optimizer Counts: Adam: 582, RMSprop: 1671, Adagrad: 783, SGD: 747, Momentum: 907\n",
      "Test Accuracy: 92.72%\n",
      "Episode 44, Avg Loss: 1.3776500290263691, Episode mean Reward: 0.0003899923348583089\n",
      "Optimizer Counts: Adam: 637, RMSprop: 1683, Adagrad: 760, SGD: 719, Momentum: 891\n",
      "Test Accuracy: 91.22%\n",
      "Episode 45, Avg Loss: 1.3893559991042497, Episode mean Reward: 0.0004486233242484175\n",
      "Optimizer Counts: Adam: 582, RMSprop: 1749, Adagrad: 702, SGD: 732, Momentum: 925\n",
      "Test Accuracy: 91.63%\n",
      "Episode 46, Avg Loss: 1.4717761958331697, Episode mean Reward: 0.00045510297897739083\n",
      "Optimizer Counts: Adam: 608, RMSprop: 1774, Adagrad: 705, SGD: 672, Momentum: 931\n",
      "Test Accuracy: 91.37%\n",
      "Episode 47, Avg Loss: 1.5039795970484646, Episode mean Reward: 0.0005377512233873627\n",
      "Optimizer Counts: Adam: 587, RMSprop: 1863, Adagrad: 764, SGD: 703, Momentum: 773\n",
      "Test Accuracy: 94.46%\n",
      "Episode 48, Avg Loss: 1.3597675799052598, Episode mean Reward: 0.00047149523302120265\n",
      "Optimizer Counts: Adam: 593, RMSprop: 1821, Adagrad: 706, SGD: 696, Momentum: 874\n",
      "Test Accuracy: 92.26%\n",
      "Episode 49, Avg Loss: 1.4188318436969318, Episode mean Reward: 0.00022340769826984398\n",
      "Optimizer Counts: Adam: 570, RMSprop: 1801, Adagrad: 730, SGD: 660, Momentum: 929\n",
      "Test Accuracy: 92.92%\n",
      "Episode 50, Avg Loss: 1.4470733760389438, Episode mean Reward: 0.00028057239174912965\n",
      "Optimizer Counts: Adam: 560, RMSprop: 2016, Adagrad: 684, SGD: 628, Momentum: 802\n",
      "Test Accuracy: 91.68%\n",
      "Episode 51, Avg Loss: 1.4919344291122738, Episode mean Reward: 0.00037148394376617657\n",
      "Optimizer Counts: Adam: 580, RMSprop: 1951, Adagrad: 721, SGD: 675, Momentum: 763\n",
      "Test Accuracy: 93.76%\n",
      "Episode 52, Avg Loss: 1.4188479894005668, Episode mean Reward: 0.000279277301297666\n",
      "Optimizer Counts: Adam: 620, RMSprop: 1771, Adagrad: 746, SGD: 613, Momentum: 940\n",
      "Test Accuracy: 92.56%\n",
      "Episode 53, Avg Loss: 1.4844638675260646, Episode mean Reward: 0.00032480698518319996\n",
      "Optimizer Counts: Adam: 527, RMSprop: 2018, Adagrad: 652, SGD: 582, Momentum: 911\n",
      "Test Accuracy: 91.76%\n",
      "Episode 54, Avg Loss: 1.5003868302429664, Episode mean Reward: 0.0003232374923061998\n",
      "Optimizer Counts: Adam: 550, RMSprop: 1951, Adagrad: 738, SGD: 626, Momentum: 825\n",
      "Test Accuracy: 91.09%\n",
      "Episode 55, Avg Loss: 1.4555498707777401, Episode mean Reward: 0.0003099427392743992\n",
      "Optimizer Counts: Adam: 517, RMSprop: 1856, Adagrad: 750, SGD: 653, Momentum: 914\n",
      "Test Accuracy: 92.98%\n",
      "Episode 56, Avg Loss: 1.4245307193763221, Episode mean Reward: 0.00018898834629187957\n",
      "Optimizer Counts: Adam: 513, RMSprop: 1835, Adagrad: 641, SGD: 717, Momentum: 984\n",
      "Test Accuracy: 92.97%\n",
      "Episode 57, Avg Loss: 1.388199981151105, Episode mean Reward: 0.00039478345426188674\n",
      "Optimizer Counts: Adam: 501, RMSprop: 1903, Adagrad: 650, SGD: 714, Momentum: 922\n",
      "Test Accuracy: 92.68%\n",
      "Episode 58, Avg Loss: 1.4403893744132157, Episode mean Reward: 2.750699660447679e-05\n",
      "Optimizer Counts: Adam: 535, RMSprop: 1984, Adagrad: 681, SGD: 604, Momentum: 886\n",
      "Test Accuracy: 93.39%\n",
      "Episode 59, Avg Loss: 1.405954453139417, Episode mean Reward: 0.0005112121583404965\n",
      "Optimizer Counts: Adam: 543, RMSprop: 2075, Adagrad: 614, SGD: 636, Momentum: 822\n",
      "Test Accuracy: 93.77%\n",
      "Episode 60, Avg Loss: 1.4061287070134048, Episode mean Reward: 0.0004598537818243269\n",
      "Optimizer Counts: Adam: 541, RMSprop: 1912, Adagrad: 643, SGD: 689, Momentum: 905\n",
      "Test Accuracy: 93.15%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = OptimizerEnv(device, batch_size)\n",
    "# agent = DQNAgent(state_size=5856, action_size=5, device=device)\n",
    "agent = DQNAgent(state_size=203542, action_size=5, device=device)\n",
    "\n",
    "loss_history = []\n",
    "num_episodes = 1000  \n",
    "\n",
    "# 🔥 옵티마이저 선택 횟수 기록\n",
    "optimizer_names = [\"Adam\", \"RMSprop\", \"Adagrad\", \"SGD\", \"Momentum\"]\n",
    "\n",
    "reward_history = []  # 🔥 각 episode의 총 reward 저장 리스트\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_losses = []\n",
    "    episode_trajectory = []\n",
    "    total_reward = 0  # 🔥 각 episode의 총 reward 초기화\n",
    "    \n",
    "    optimizer_count = {i: 0 for i in range(5)}  # 🔥 옵티마이저 선택 횟수 초기화\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)  # 🔥 매 batch마다 옵티마이저 선택\n",
    "        optimizer_count[action] += 1  # 🔥 선택된 옵티마이저 카운트 증가\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # next_state, reward, done, _ = env.step()\n",
    "\n",
    "        total_reward += reward  # 🔥 reward를 episode 단위로 누적\n",
    "        episode_losses.append(state[1])  \n",
    "        episode_trajectory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    for transition in episode_trajectory:\n",
    "        agent.remember(*transition)\n",
    "\n",
    "    avg_loss = sum(episode_losses) / len(episode_losses)\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "    avg_reward = total_reward / len(episode_trajectory) if episode_trajectory else 0\n",
    "    reward_history.append(avg_reward)  # 🔥 episode 종료 후 총 reward 저장\n",
    "\n",
    "    agent.train(batch_size)\n",
    "\n",
    "    print(f\"Episode {episode+1}, Avg Loss: {avg_loss}, Episode mean Reward: {avg_reward}\")\n",
    "    print(f\"Optimizer Counts: {', '.join([f'{optimizer_names[i]}: {optimizer_count[i]}' for i in range(5)])}\")\n",
    "\n",
    "    model = env.model\n",
    "    # 모델 정확성 평가\n",
    "    evaluate_model(model, testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(loss_history, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL2O Optimizer Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(loss_history, label=\"L2O Optimizer Loss\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Reduction Over Training Episodes\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward 수정\n",
    "- discount rate 적용해보기 --> 이게 gamma값\n",
    "- moving average 방식\n",
    "- 배치단위 리워드 -> 에폭 단위 리워드로 변경?\n",
    "\n",
    "\n",
    "state 수정\n",
    "- 파라미터 값 추가\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
