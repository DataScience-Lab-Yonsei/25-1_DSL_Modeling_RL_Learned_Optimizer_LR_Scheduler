{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "import torch.amp as amp\n",
    "\n",
    "data_path = \"C:/Users/ksw00/mycode/data\"\n",
    "# âœ… GPU ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "\n",
    "def get_dataloader(batch_size, train):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    if train:\n",
    "        # MNIST ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "        mnist_dataset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n",
    "        data_loader = DataLoader(mnist_dataset, batch_size=len(mnist_dataset), shuffle=False)\n",
    "    else:\n",
    "        # MNIST ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "        test_dataset = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)\n",
    "        data_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    \n",
    "    # # RP\n",
    "    # data_iter = iter(data_loader)\n",
    "    # images, labels = next(data_iter)\n",
    "    # images = images.view(images.size(0), -1).numpy()\n",
    "    \n",
    "    # rp = GaussianRandomProjection(n_components=784)\n",
    "    # images_reduced = rp.fit_transform(images)\n",
    "\n",
    "    # eps = 1e-8\n",
    "    # images_reduced = (images_reduced - np.mean(images_reduced, axis=0)) / (np.std(images_reduced, axis=0) + eps)\n",
    "\n",
    "    # images_reduced = torch.tensor(images_reduced, dtype=torch.float32)\n",
    "    # labels = labels.clone().detach().long()\n",
    "\n",
    "    # dataset = TensorDataset(images_reduced, labels)\n",
    "    # data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=train)\n",
    "\n",
    "\n",
    "    # No RP\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    dataset = TensorDataset(images, labels)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle = train)\n",
    "    \n",
    "    return data_loader\n",
    "\n",
    "testloader = get_dataloader(batch_size, train=False)\n",
    "\n",
    "# ëª¨ë¸ í‰ê°€ í•¨ìˆ˜\n",
    "def evaluate_model(model, testloader):\n",
    "    model.eval()  # í‰ê°€ ëª¨ë“œ í™œì„±í™”\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.view(images.size(0), -1).to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim= 48, hidden_dim= 48, output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        with amp.autocast(device_type=str(device)):   \n",
    "            x = self.fc1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class MLP2(nn.Module):\n",
    "    def __init__(self, input_dim= 28*28, hidden_dim= 128, output_dim=10):\n",
    "        super(MLP2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):    \n",
    "        with amp.autocast(device_type=str(device)): \n",
    "            x = self.fc1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerEnv(gym.Env):\n",
    "    def __init__(self, device, batch_size):\n",
    "        super(OptimizerEnv, self).__init__()\n",
    "        self.device = device\n",
    "        self.train_loader = get_dataloader(batch_size, train=True)\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(5)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "        # 2ì¸µ MLP ëª¨ë¸\n",
    "        # self.model = MLP().to(self.device)\n",
    "        self.model = MLP2().to(self.device)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.episode_step = 0\n",
    "        self.init_loss = None  \n",
    "        self.optimizer = None  \n",
    "        self.loss_history = []\n",
    "        # ğŸ”¥ AMP GradScaler ì¶”ê°€\n",
    "        self.scaler = amp.GradScaler()\n",
    "        # ğŸ”¥ ë°ì´í„° ë¡œë” ì´í„°ë ˆì´í„° ì„¤ì • (ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ë¥¼ ìœ„í•´)\n",
    "        self.train_loader_iter = iter(self.train_loader)\n",
    "\n",
    "    def step(self, action):\n",
    "        self._set_optimizer(action)\n",
    "        \"\"\"ğŸ”¥ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì˜µí‹°ë§ˆì´ì € ì„ íƒ\"\"\"\n",
    "    # def step(self):     \n",
    "    #     self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.RMSprop(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.Adagrad(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.SGD(self.model.parameters(), lr=0.001)\n",
    "        # self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        \n",
    "        try:\n",
    "            images, labels = next(self.train_loader_iter)  # ğŸ”¥ ë‹¤ìŒ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°\n",
    "        except StopIteration:\n",
    "            # ğŸ”¥ ë°°ì¹˜ ë‹¤ ëŒë©´ ìƒˆë¡œìš´ epoch ì‹œì‘\n",
    "            self.train_loader_iter = iter(self.train_loader)\n",
    "            images, labels = next(self.train_loader_iter)\n",
    "\n",
    "        images, labels = images.view(images.size(0), -1).to(self.device), labels.to(self.device)\n",
    "\n",
    "        # âœ… ìë™ í˜¼í•© ì •ë°€ë„ ì ìš©\n",
    "        with amp.autocast(device_type=str(self.device)):\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        self.loss_history.append(loss_value)\n",
    "        moving_avg_loss = np.mean(self.loss_history[-10:]) if self.loss_history else 0\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # âœ… Scalerë¥¼ ì‚¬ìš©í•˜ì—¬ ì—­ì „íŒŒ\n",
    "        self.scaler.scale(loss).backward()\n",
    "\n",
    "        # ğŸ”¥ MLP ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
    "        model_params = torch.cat([p.flatten() for p in self.model.parameters()]).detach().cpu().numpy()\n",
    "\n",
    "        # ğŸ”¥ MLP ëª¨ë¸ì˜ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€)\n",
    "        model_grads = []\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                model_grads.append(p.grad.flatten())\n",
    "            else:\n",
    "                model_grads.append(torch.zeros_like(p.flatten()))  # ğŸ”¥ Noneì´ë©´ 0ìœ¼ë¡œ ì±„ì›€\n",
    "        model_grads = torch.cat(model_grads).detach().cpu().numpy()\n",
    "        model_grads = np.nan_to_num(model_grads, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        \n",
    "        # ğŸ”¥ stateì— ì¶•ì†Œëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë° ê·¸ë¼ë””ì–¸íŠ¸ ì¶”ê°€\n",
    "        state = np.concatenate(([self.episode_step, moving_avg_loss], model_params, model_grads)).astype(np.float32)\n",
    "        self.episode_step += 1\n",
    "\n",
    "        if self.init_loss is None:\n",
    "            self.init_loss = loss_value # ğŸ”¥ ì´ˆê¸° ì†ì‹¤ê°’ ì €ì¥\n",
    "\n",
    "        reward = -np.log(loss_value / self.init_loss) / (self.episode_step - 1) if self.episode_step > 1 else 0\n",
    "            \n",
    "        done = self.episode_step >= 10 * len(self.train_loader)  # ğŸ”¥ ì „ì²´ ë°°ì¹˜ ìˆ˜ ê³ ë ¤í•œ ì¢…ë£Œ ì¡°ê±´\n",
    "        \n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"í™˜ê²½ ì´ˆê¸°í™”\"\"\"\n",
    "        self.episode_step = 0\n",
    "        self.init_loss = None\n",
    "        self.loss_history = []\n",
    "        self.model.apply(self._reset_weights)\n",
    "        self.train_loader_iter = iter(self.train_loader)\n",
    "\n",
    "        # ğŸ”¥ ì´ˆê¸° ê°€ì¤‘ì¹˜ ë° ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ 0ìœ¼ë¡œ ì„¤ì • (ì™„ì „ ì´ˆê¸°í™”)\n",
    "        # model_params = np.zeros(2842, dtype=np.float32)  # ê°€ì¤‘ì¹˜ ì´ˆê¸°ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •\n",
    "        # model_grads = np.zeros(2842, dtype=np.float32)  # ì´ˆê¸°ì—ëŠ” ê·¸ë¼ë””ì–¸íŠ¸ ì—†ìŒ\n",
    "\n",
    "        model_params = np.zeros(101770, dtype=np.float32)  # ê°€ì¤‘ì¹˜ ì´ˆê¸°ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •\n",
    "        model_grads = np.zeros(101770, dtype=np.float32)  # ì´ˆê¸°ì—ëŠ” ê·¸ë¼ë””ì–¸íŠ¸ ì—†ìŒ\n",
    "\n",
    "        # ğŸ”¥ stateì— ì¶•ì†Œëœ ê°€ì¤‘ì¹˜ì™€ ì´ˆê¸° ê·¸ë¼ë””ì–¸íŠ¸ í¬í•¨\n",
    "        state = np.concatenate(([self.episode_step, 0.0], model_params, model_grads)).astype(np.float32)\n",
    "        return state\n",
    "\n",
    "    def _set_optimizer(self, action):\n",
    "        \"\"\"ë§¤ ë°°ì¹˜ë§ˆë‹¤ ì˜µí‹°ë§ˆì´ì € ë³€ê²½\"\"\"\n",
    "        optimizers = [\n",
    "            optim.Adam(self.model.parameters(), lr=0.001),\n",
    "            optim.RMSprop(self.model.parameters(), lr=0.001),\n",
    "            optim.Adagrad(self.model.parameters(), lr=0.001),\n",
    "            optim.SGD(self.model.parameters(), lr=0.001),\n",
    "            optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        ]\n",
    "        self.optimizer = optimizers[action]\n",
    "    \n",
    "    def _reset_weights(self, m):\n",
    "        \"\"\"ëª¨ë¸ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, device):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = device\n",
    "\n",
    "        self.epsilon = 1.0  \n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99  # ğŸ”¥ íƒìƒ‰ ë¹„ìœ¨ ê°ì†Œ ì¡°ì •\n",
    "        self.gamma = 0.9  # ğŸ”¥ ë‹¨ê¸°ì  ë³´ìƒ ê³ ë ¤ ê°•í™”\n",
    "        self.lr = 0.001\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_size)\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.memory = []\n",
    "        self.memory_size = 5000  # ğŸ”¥ ê²½í—˜ ì €ì¥ í¬ê¸° ì„¤ì •\n",
    "        self.memory = deque(maxlen=self.memory_size)  # ğŸ”¥ ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°ë¥¼ ìë™ ì‚­ì œ\n",
    "\n",
    "        self.scaler = amp.GradScaler()\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Îµ-greedy ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.model(state)).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"ê²½í—˜ ì €ì¥\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self, batch_size=128):\n",
    "        \"\"\"DQN í•™ìŠµ\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "\n",
    "        with amp.autocast(device_type=str(self.device)):\n",
    "            q_values = self.model(states).gather(1, actions)\n",
    "            next_q_values = self.model(next_states).max(1, keepdim=True)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "            loss = F.mse_loss(q_values, target_q_values.detach())\n",
    "\n",
    "        # âœ… Scaler ì ìš©\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Avg Loss: 1.231890397234512, Episode mean Reward: 0.0006057805069635702\n",
      "Optimizer Counts: Adam: 980, RMSprop: 928, Adagrad: 967, SGD: 936, Momentum: 879\n",
      "Test Accuracy: 92.22%\n",
      "Episode 2, Avg Loss: 1.2956901462665245, Episode mean Reward: 0.0004323564197591837\n",
      "Optimizer Counts: Adam: 906, RMSprop: 983, Adagrad: 952, SGD: 921, Momentum: 928\n",
      "Test Accuracy: 94.09%\n",
      "Episode 3, Avg Loss: 1.1941080491680072, Episode mean Reward: 0.0005806761317231226\n",
      "Optimizer Counts: Adam: 915, RMSprop: 987, Adagrad: 907, SGD: 960, Momentum: 921\n",
      "Test Accuracy: 92.13%\n",
      "Episode 4, Avg Loss: 1.3232380460002529, Episode mean Reward: 0.0004644261116003281\n",
      "Optimizer Counts: Adam: 887, RMSprop: 1028, Adagrad: 911, SGD: 935, Momentum: 929\n",
      "Test Accuracy: 93.68%\n",
      "Episode 5, Avg Loss: 1.2564257536425012, Episode mean Reward: 0.000668385652896342\n",
      "Optimizer Counts: Adam: 908, RMSprop: 1056, Adagrad: 879, SGD: 911, Momentum: 936\n",
      "Test Accuracy: 93.21%\n",
      "Episode 6, Avg Loss: 1.1656318811401885, Episode mean Reward: 0.0004243769963253888\n",
      "Optimizer Counts: Adam: 870, RMSprop: 1039, Adagrad: 886, SGD: 898, Momentum: 997\n",
      "Test Accuracy: 92.52%\n",
      "Episode 7, Avg Loss: 1.2246570794948384, Episode mean Reward: 0.0004860107286225843\n",
      "Optimizer Counts: Adam: 892, RMSprop: 1054, Adagrad: 947, SGD: 879, Momentum: 918\n",
      "Test Accuracy: 87.07%\n",
      "Episode 8, Avg Loss: 1.3649045997464073, Episode mean Reward: 0.00040825406120742177\n",
      "Optimizer Counts: Adam: 865, RMSprop: 1071, Adagrad: 955, SGD: 857, Momentum: 942\n",
      "Test Accuracy: 88.94%\n",
      "Episode 9, Avg Loss: 1.3063602090390252, Episode mean Reward: 0.000521181281444042\n",
      "Optimizer Counts: Adam: 843, RMSprop: 1148, Adagrad: 917, SGD: 844, Momentum: 938\n",
      "Test Accuracy: 93.53%\n",
      "Episode 10, Avg Loss: 1.2577476267875638, Episode mean Reward: 0.00038048592754753624\n",
      "Optimizer Counts: Adam: 902, RMSprop: 1135, Adagrad: 907, SGD: 845, Momentum: 901\n",
      "Test Accuracy: 93.29%\n",
      "Episode 11, Avg Loss: 1.348777549099058, Episode mean Reward: 0.00040874451087057463\n",
      "Optimizer Counts: Adam: 834, RMSprop: 1087, Adagrad: 929, SGD: 938, Momentum: 902\n",
      "Test Accuracy: 92.09%\n",
      "Episode 12, Avg Loss: 1.2937167333323818, Episode mean Reward: 0.0006444224874379892\n",
      "Optimizer Counts: Adam: 857, RMSprop: 1162, Adagrad: 862, SGD: 871, Momentum: 938\n",
      "Test Accuracy: 90.69%\n",
      "Episode 13, Avg Loss: 1.380621165151535, Episode mean Reward: 0.00039720514449793524\n",
      "Optimizer Counts: Adam: 814, RMSprop: 1193, Adagrad: 890, SGD: 876, Momentum: 917\n",
      "Test Accuracy: 93.13%\n",
      "Episode 14, Avg Loss: 1.3459083367385336, Episode mean Reward: 0.00031183032864524305\n",
      "Optimizer Counts: Adam: 814, RMSprop: 1232, Adagrad: 843, SGD: 876, Momentum: 925\n",
      "Test Accuracy: 92.22%\n",
      "Episode 15, Avg Loss: 1.3226846107668968, Episode mean Reward: 0.00046142022395140997\n",
      "Optimizer Counts: Adam: 783, RMSprop: 1238, Adagrad: 871, SGD: 876, Momentum: 922\n",
      "Test Accuracy: 93.29%\n",
      "Episode 16, Avg Loss: 1.3708469572860296, Episode mean Reward: 0.00026449063805236104\n",
      "Optimizer Counts: Adam: 813, RMSprop: 1305, Adagrad: 852, SGD: 832, Momentum: 888\n",
      "Test Accuracy: 91.97%\n",
      "Episode 17, Avg Loss: 1.337719859637177, Episode mean Reward: 0.0005647927159366634\n",
      "Optimizer Counts: Adam: 808, RMSprop: 1215, Adagrad: 838, SGD: 885, Momentum: 944\n",
      "Test Accuracy: 93.63%\n",
      "Episode 18, Avg Loss: 1.2982057254832944, Episode mean Reward: 0.0005015485262273918\n",
      "Optimizer Counts: Adam: 762, RMSprop: 1364, Adagrad: 851, SGD: 777, Momentum: 936\n",
      "Test Accuracy: 93.05%\n",
      "Episode 19, Avg Loss: 1.3736161130831948, Episode mean Reward: 0.00037023132638008946\n",
      "Optimizer Counts: Adam: 775, RMSprop: 1380, Adagrad: 803, SGD: 844, Momentum: 888\n",
      "Test Accuracy: 93.30%\n",
      "Episode 20, Avg Loss: 1.4920249864744988, Episode mean Reward: 0.0004056132046139953\n",
      "Optimizer Counts: Adam: 796, RMSprop: 1390, Adagrad: 832, SGD: 806, Momentum: 866\n",
      "Test Accuracy: 90.96%\n",
      "Episode 21, Avg Loss: 1.3874303504792866, Episode mean Reward: 0.0004086178466729054\n",
      "Optimizer Counts: Adam: 742, RMSprop: 1391, Adagrad: 857, SGD: 771, Momentum: 929\n",
      "Test Accuracy: 92.97%\n",
      "Episode 22, Avg Loss: 1.261071369236212, Episode mean Reward: 0.0002983058192363945\n",
      "Optimizer Counts: Adam: 731, RMSprop: 1334, Adagrad: 852, SGD: 804, Momentum: 969\n",
      "Test Accuracy: 90.77%\n",
      "Episode 23, Avg Loss: 1.325928490680418, Episode mean Reward: 0.00033115035092869523\n",
      "Optimizer Counts: Adam: 795, RMSprop: 1417, Adagrad: 785, SGD: 808, Momentum: 885\n",
      "Test Accuracy: 91.04%\n",
      "Episode 24, Avg Loss: 1.4472479721249294, Episode mean Reward: 0.00036853520108935596\n",
      "Optimizer Counts: Adam: 741, RMSprop: 1399, Adagrad: 856, SGD: 830, Momentum: 864\n",
      "Test Accuracy: 93.58%\n",
      "Episode 25, Avg Loss: 1.4086529437539927, Episode mean Reward: 0.00020915181439587203\n",
      "Optimizer Counts: Adam: 752, RMSprop: 1418, Adagrad: 761, SGD: 785, Momentum: 974\n",
      "Test Accuracy: 94.01%\n",
      "Episode 26, Avg Loss: 1.4422199755843514, Episode mean Reward: 8.522912097472804e-05\n",
      "Optimizer Counts: Adam: 740, RMSprop: 1515, Adagrad: 770, SGD: 810, Momentum: 855\n",
      "Test Accuracy: 92.61%\n",
      "Episode 27, Avg Loss: 1.3711939559562374, Episode mean Reward: 0.0002587134201291819\n",
      "Optimizer Counts: Adam: 785, RMSprop: 1453, Adagrad: 767, SGD: 753, Momentum: 932\n",
      "Test Accuracy: 93.01%\n",
      "Episode 28, Avg Loss: 1.4051825591114793, Episode mean Reward: -1.4754974573465353e-05\n",
      "Optimizer Counts: Adam: 713, RMSprop: 1492, Adagrad: 806, SGD: 751, Momentum: 928\n",
      "Test Accuracy: 92.23%\n",
      "Episode 29, Avg Loss: 1.4530678713753789, Episode mean Reward: 0.0004632701919314188\n",
      "Optimizer Counts: Adam: 730, RMSprop: 1563, Adagrad: 755, SGD: 715, Momentum: 927\n",
      "Test Accuracy: 93.23%\n",
      "Episode 30, Avg Loss: 1.3419481252683505, Episode mean Reward: 0.00031959572680985903\n",
      "Optimizer Counts: Adam: 747, RMSprop: 1476, Adagrad: 792, SGD: 751, Momentum: 924\n",
      "Test Accuracy: 91.21%\n",
      "Episode 31, Avg Loss: 1.4002132511342258, Episode mean Reward: 0.00045607057072782827\n",
      "Optimizer Counts: Adam: 715, RMSprop: 1510, Adagrad: 735, SGD: 730, Momentum: 1000\n",
      "Test Accuracy: 89.69%\n",
      "Episode 32, Avg Loss: 1.3856278396999913, Episode mean Reward: 0.00023171092501867598\n",
      "Optimizer Counts: Adam: 688, RMSprop: 1520, Adagrad: 748, SGD: 817, Momentum: 917\n",
      "Test Accuracy: 92.41%\n",
      "Episode 33, Avg Loss: 1.4294709054010508, Episode mean Reward: 0.0003201267470967555\n",
      "Optimizer Counts: Adam: 648, RMSprop: 1626, Adagrad: 764, SGD: 744, Momentum: 908\n",
      "Test Accuracy: 94.42%\n",
      "Episode 34, Avg Loss: 1.4145870197302244, Episode mean Reward: 0.0003000192577539492\n",
      "Optimizer Counts: Adam: 679, RMSprop: 1645, Adagrad: 756, SGD: 738, Momentum: 872\n",
      "Test Accuracy: 91.28%\n",
      "Episode 35, Avg Loss: 1.4033390127138288, Episode mean Reward: 0.00035978667934205176\n",
      "Optimizer Counts: Adam: 689, RMSprop: 1623, Adagrad: 774, SGD: 720, Momentum: 884\n",
      "Test Accuracy: 92.18%\n",
      "Episode 36, Avg Loss: 1.4344588117241097, Episode mean Reward: 0.0007214664043529631\n",
      "Optimizer Counts: Adam: 608, RMSprop: 1626, Adagrad: 765, SGD: 729, Momentum: 962\n",
      "Test Accuracy: 92.48%\n",
      "Episode 37, Avg Loss: 1.3487939170428684, Episode mean Reward: 0.0003751898447766742\n",
      "Optimizer Counts: Adam: 663, RMSprop: 1632, Adagrad: 798, SGD: 665, Momentum: 932\n",
      "Test Accuracy: 88.00%\n",
      "Episode 38, Avg Loss: 1.399988896112198, Episode mean Reward: 0.00047234542257482034\n",
      "Optimizer Counts: Adam: 610, RMSprop: 1639, Adagrad: 715, SGD: 716, Momentum: 1010\n",
      "Test Accuracy: 93.40%\n",
      "Episode 39, Avg Loss: 1.3982080402913124, Episode mean Reward: 0.0002794028443346348\n",
      "Optimizer Counts: Adam: 643, RMSprop: 1636, Adagrad: 806, SGD: 720, Momentum: 885\n",
      "Test Accuracy: 90.24%\n",
      "Episode 40, Avg Loss: 1.3791176057954841, Episode mean Reward: 0.00012493456864151324\n",
      "Optimizer Counts: Adam: 580, RMSprop: 1696, Adagrad: 803, SGD: 710, Momentum: 901\n",
      "Test Accuracy: 92.75%\n",
      "Episode 41, Avg Loss: 1.472670358254203, Episode mean Reward: 0.00019106057663052957\n",
      "Optimizer Counts: Adam: 683, RMSprop: 1726, Adagrad: 746, SGD: 685, Momentum: 850\n",
      "Test Accuracy: 93.30%\n",
      "Episode 42, Avg Loss: 1.4439716819252795, Episode mean Reward: 0.0004531268112083105\n",
      "Optimizer Counts: Adam: 587, RMSprop: 1852, Adagrad: 677, SGD: 744, Momentum: 830\n",
      "Test Accuracy: 94.00%\n",
      "Episode 43, Avg Loss: 1.4611313701311408, Episode mean Reward: 0.00042258208199074906\n",
      "Optimizer Counts: Adam: 582, RMSprop: 1671, Adagrad: 783, SGD: 747, Momentum: 907\n",
      "Test Accuracy: 92.72%\n",
      "Episode 44, Avg Loss: 1.3776500290263691, Episode mean Reward: 0.0003899923348583089\n",
      "Optimizer Counts: Adam: 637, RMSprop: 1683, Adagrad: 760, SGD: 719, Momentum: 891\n",
      "Test Accuracy: 91.22%\n",
      "Episode 45, Avg Loss: 1.3893559991042497, Episode mean Reward: 0.0004486233242484175\n",
      "Optimizer Counts: Adam: 582, RMSprop: 1749, Adagrad: 702, SGD: 732, Momentum: 925\n",
      "Test Accuracy: 91.63%\n",
      "Episode 46, Avg Loss: 1.4717761958331697, Episode mean Reward: 0.00045510297897739083\n",
      "Optimizer Counts: Adam: 608, RMSprop: 1774, Adagrad: 705, SGD: 672, Momentum: 931\n",
      "Test Accuracy: 91.37%\n",
      "Episode 47, Avg Loss: 1.5039795970484646, Episode mean Reward: 0.0005377512233873627\n",
      "Optimizer Counts: Adam: 587, RMSprop: 1863, Adagrad: 764, SGD: 703, Momentum: 773\n",
      "Test Accuracy: 94.46%\n",
      "Episode 48, Avg Loss: 1.3597675799052598, Episode mean Reward: 0.00047149523302120265\n",
      "Optimizer Counts: Adam: 593, RMSprop: 1821, Adagrad: 706, SGD: 696, Momentum: 874\n",
      "Test Accuracy: 92.26%\n",
      "Episode 49, Avg Loss: 1.4188318436969318, Episode mean Reward: 0.00022340769826984398\n",
      "Optimizer Counts: Adam: 570, RMSprop: 1801, Adagrad: 730, SGD: 660, Momentum: 929\n",
      "Test Accuracy: 92.92%\n",
      "Episode 50, Avg Loss: 1.4470733760389438, Episode mean Reward: 0.00028057239174912965\n",
      "Optimizer Counts: Adam: 560, RMSprop: 2016, Adagrad: 684, SGD: 628, Momentum: 802\n",
      "Test Accuracy: 91.68%\n",
      "Episode 51, Avg Loss: 1.4919344291122738, Episode mean Reward: 0.00037148394376617657\n",
      "Optimizer Counts: Adam: 580, RMSprop: 1951, Adagrad: 721, SGD: 675, Momentum: 763\n",
      "Test Accuracy: 93.76%\n",
      "Episode 52, Avg Loss: 1.4188479894005668, Episode mean Reward: 0.000279277301297666\n",
      "Optimizer Counts: Adam: 620, RMSprop: 1771, Adagrad: 746, SGD: 613, Momentum: 940\n",
      "Test Accuracy: 92.56%\n",
      "Episode 53, Avg Loss: 1.4844638675260646, Episode mean Reward: 0.00032480698518319996\n",
      "Optimizer Counts: Adam: 527, RMSprop: 2018, Adagrad: 652, SGD: 582, Momentum: 911\n",
      "Test Accuracy: 91.76%\n",
      "Episode 54, Avg Loss: 1.5003868302429664, Episode mean Reward: 0.0003232374923061998\n",
      "Optimizer Counts: Adam: 550, RMSprop: 1951, Adagrad: 738, SGD: 626, Momentum: 825\n",
      "Test Accuracy: 91.09%\n",
      "Episode 55, Avg Loss: 1.4555498707777401, Episode mean Reward: 0.0003099427392743992\n",
      "Optimizer Counts: Adam: 517, RMSprop: 1856, Adagrad: 750, SGD: 653, Momentum: 914\n",
      "Test Accuracy: 92.98%\n",
      "Episode 56, Avg Loss: 1.4245307193763221, Episode mean Reward: 0.00018898834629187957\n",
      "Optimizer Counts: Adam: 513, RMSprop: 1835, Adagrad: 641, SGD: 717, Momentum: 984\n",
      "Test Accuracy: 92.97%\n",
      "Episode 57, Avg Loss: 1.388199981151105, Episode mean Reward: 0.00039478345426188674\n",
      "Optimizer Counts: Adam: 501, RMSprop: 1903, Adagrad: 650, SGD: 714, Momentum: 922\n",
      "Test Accuracy: 92.68%\n",
      "Episode 58, Avg Loss: 1.4403893744132157, Episode mean Reward: 2.750699660447679e-05\n",
      "Optimizer Counts: Adam: 535, RMSprop: 1984, Adagrad: 681, SGD: 604, Momentum: 886\n",
      "Test Accuracy: 93.39%\n",
      "Episode 59, Avg Loss: 1.405954453139417, Episode mean Reward: 0.0005112121583404965\n",
      "Optimizer Counts: Adam: 543, RMSprop: 2075, Adagrad: 614, SGD: 636, Momentum: 822\n",
      "Test Accuracy: 93.77%\n",
      "Episode 60, Avg Loss: 1.4061287070134048, Episode mean Reward: 0.0004598537818243269\n",
      "Optimizer Counts: Adam: 541, RMSprop: 1912, Adagrad: 643, SGD: 689, Momentum: 905\n",
      "Test Accuracy: 93.15%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = OptimizerEnv(device, batch_size)\n",
    "# agent = DQNAgent(state_size=5856, action_size=5, device=device)\n",
    "agent = DQNAgent(state_size=203542, action_size=5, device=device)\n",
    "\n",
    "loss_history = []\n",
    "num_episodes = 1000  \n",
    "\n",
    "# ğŸ”¥ ì˜µí‹°ë§ˆì´ì € ì„ íƒ íšŸìˆ˜ ê¸°ë¡\n",
    "optimizer_names = [\"Adam\", \"RMSprop\", \"Adagrad\", \"SGD\", \"Momentum\"]\n",
    "\n",
    "reward_history = []  # ğŸ”¥ ê° episodeì˜ ì´ reward ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_losses = []\n",
    "    episode_trajectory = []\n",
    "    total_reward = 0  # ğŸ”¥ ê° episodeì˜ ì´ reward ì´ˆê¸°í™”\n",
    "    \n",
    "    optimizer_count = {i: 0 for i in range(5)}  # ğŸ”¥ ì˜µí‹°ë§ˆì´ì € ì„ íƒ íšŸìˆ˜ ì´ˆê¸°í™”\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)  # ğŸ”¥ ë§¤ batchë§ˆë‹¤ ì˜µí‹°ë§ˆì´ì € ì„ íƒ\n",
    "        optimizer_count[action] += 1  # ğŸ”¥ ì„ íƒëœ ì˜µí‹°ë§ˆì´ì € ì¹´ìš´íŠ¸ ì¦ê°€\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # next_state, reward, done, _ = env.step()\n",
    "\n",
    "        total_reward += reward  # ğŸ”¥ rewardë¥¼ episode ë‹¨ìœ„ë¡œ ëˆ„ì \n",
    "        episode_losses.append(state[1])  \n",
    "        episode_trajectory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    for transition in episode_trajectory:\n",
    "        agent.remember(*transition)\n",
    "\n",
    "    avg_loss = sum(episode_losses) / len(episode_losses)\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "    avg_reward = total_reward / len(episode_trajectory) if episode_trajectory else 0\n",
    "    reward_history.append(avg_reward)  # ğŸ”¥ episode ì¢…ë£Œ í›„ ì´ reward ì €ì¥\n",
    "\n",
    "    agent.train(batch_size)\n",
    "\n",
    "    print(f\"Episode {episode+1}, Avg Loss: {avg_loss}, Episode mean Reward: {avg_reward}\")\n",
    "    print(f\"Optimizer Counts: {', '.join([f'{optimizer_names[i]}: {optimizer_count[i]}' for i in range(5)])}\")\n",
    "\n",
    "    model = env.model\n",
    "    # ëª¨ë¸ ì •í™•ì„± í‰ê°€\n",
    "    evaluate_model(model, testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(loss_history, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL2O Optimizer Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(loss_history, label=\"L2O Optimizer Loss\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Reduction Over Training Episodes\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward ìˆ˜ì •\n",
    "- discount rate ì ìš©í•´ë³´ê¸° --> ì´ê²Œ gammaê°’\n",
    "- moving average ë°©ì‹\n",
    "- ë°°ì¹˜ë‹¨ìœ„ ë¦¬ì›Œë“œ -> ì—í­ ë‹¨ìœ„ ë¦¬ì›Œë“œë¡œ ë³€ê²½?\n",
    "\n",
    "\n",
    "state ìˆ˜ì •\n",
    "- íŒŒë¼ë¯¸í„° ê°’ ì¶”ê°€\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
